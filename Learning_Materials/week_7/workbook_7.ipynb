{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 7: Artificial Neural Networks 1 - Perceptrons\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will introduce the Perceptron algorithm for supervised learning, and the building blocks of artificial neural networks.\n",
    "    - We introduce the Perceptron algorithm and you are provided a code implementation for a 2 input Perceptron.\n",
    "    - We will then show Perceptrons can learn simple logical operator functions, using binary truth-table data.\n",
    "\n",
    "2. The second part of this workbook will demonstrate Perceptron decision boundaries.\n",
    "    - Perceptrons are only capable of learning linear decision boundaries.\n",
    "    - This demonstrates the limitations of Perceptrons and why we might need to combine them into networks.\n",
    "\n",
    "3. The third part of this workbook will apply the Perceptron to real valued data.\n",
    "    - We will use the Iris dataset we introduced in the previous weeks.\n",
    "    - Then demonstrate how you might apply a simple linear classifier to multi-class data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 1: Perceptrons - The basis of Artificial Neural Networks\n",
    "\n",
    "Perceptrons, invented by Frank Rosenblatt in the late 1950's,\n",
    "are a form of supervised machine learning algorithm inspired by neuron cells.\n",
    "In neurons, signals come in along the dendrites and out along the axon. \n",
    "A synapse is the connection between the axon of one cell and the dendrites of another.\n",
    "Crudely, input signals are 'summed' and if they reach a certain threshold the neuron 'fires'\n",
    "and sends a signal down the synapse to the connected cells.\n",
    "\n",
    "![Perceptron](figures/Perceptron.png \"Perceptron Image\")\n",
    "\n",
    "Perceptrons are an algorithmic approximation of this process and can learn to solve simple classification problems.  \n",
    "Input values are multiplied by a learnable parameter called a *weight*.  \n",
    "If the sum of the inputs $\\times$ weights is over a certain threshold the Perceptron 'fires' and generates an output.  \n",
    "We use the *error* in the output to change the value of the *weights* by a small amount - the *learning rate*.  \n",
    "The process is repeated until the error is 0, or as small as we can get it.\n",
    "\n",
    "**Note:** \n",
    "- The threshold which determines if the Perceptron produces an output is determined by its *activation function*.\n",
    "- For Perceptrons this is often a step function which outputs a 1 or 0 i.e. 'fires' or not.\n",
    "- However, it can also be a non-linear function such as sigmoid (also called `logistic`), which will always produce a real numbered output in the range 0 to 1.\n",
    "\n",
    "### Perceptron - Algorithm\n",
    "\n",
    "1. Set weights to random values in range [-0.5, 0.5]\n",
    "\n",
    "2. Set learning rate to a small value, usually less than 0.5\n",
    "\n",
    "3. For each training example in the dataset i.e one 'epoch'\n",
    "\n",
    "    A. Calculate output (activation)\n",
    "    \n",
    "    $sum = \\sum\\limits_{i=0}^{n} w_i \\times x_i$\n",
    "      \n",
    "    $if\\ sum >\\ 0 \\\\ \\;\\;\\;activation = 1 $ &nbsp;&nbsp;&nbsp; $\\\\else \\\\ \\;\\;\\;activation = 0$\n",
    "       \n",
    "    B. Calculate error\n",
    "    \n",
    "    $error = target \\, output - activation$\n",
    "\n",
    "    C. Update each of the weights values\n",
    "    \n",
    "    $change \\, in \\, weight = error \\times input \\times learning \\, rate$\n",
    "\n",
    "\n",
    "4. Repeat from step 3 until error is 0 (or as close as possible), or for the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:3px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 1: Perceptrons - Logical Operators</h2>\n",
    "The following few sections create binary datasets (based on truth tables) for logical operators and then implement and train a perceptron to 'solve' the logical functions.\n",
    "\n",
    "You should try and familiarise yourself with the algorithm and how the process builds a model by learning values for the weights.\n",
    "<ol>\n",
    "    <li>Compare the pseudocode/algorithm above with the code implementation and try to understand how it learns by updating the weights.</li>\n",
    "    <li>Try changing the <code>learning_rate</code> parameter, which controls how large the change in weights is, and see how that effects learning.</li>\n",
    "    <li>Try changing the <code>target_outputs</code> to <code>target_outputs_OR</code> or <code>target_outputs_XOR</code>. For each dataset/problem:\n",
    "        <ul>\n",
    "            <li>Make a prediction about whether you will see exactly the same thing when you run the cell again.</li>\n",
    "            <li>Make a prediction about whether the perceptron will solve the problem (if not, why not?)</li>\n",
    "            <li><b>Be honest!</b>: Write down your answer, and your reasoning <i>before</i> you run the cell :)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finally, answer the Multiple Choice Questions to check your understanding</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the datasets - the logical *connective* functions\n",
    "\n",
    "We are going to use binary data to show that Perceptrons can learn to represent logical functions,\n",
    "though you could also think about it as a prediction/classification problem\n",
    "i.e. for a given set of inputs what is the correct output.\n",
    "A truth table can be used as the Perceptrons *training* data, with each row representing an input example.\n",
    "Each training example has two inputs (*features*) and one output (*label*).\n",
    "\n",
    "| Input 1| Input 2| AND label | OR label  | XOR label |\n",
    "|:------:|:------:|:---:|:---:|:---:|\n",
    "| 0      | 0      | 0   | 0   | 0   | \n",
    "| 0      | 1      | 0   | 1   | 1   |\n",
    "| 1      | 0      | 0   | 1   | 1   |\n",
    "| 1      | 1      | 1   | 1   | 0   |\n",
    "\n",
    "First we will import some python modules and then create the training data.\n",
    "\n",
    "**Note:** Input data is often denoted as X and labels/target outputs with Y.\n",
    "Here we are going to use **inputs**, but the target outputs have been labeled **AND**, **OR** and **XOR**.\n",
    "This is so we can be clear about what the outputs should be.\n",
    "\n",
    "**Run the cell below to create the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
      "target_outputs_AND: [0, 0, 0, 1]\n",
      "target_outputs_OR: [0, 1, 1, 1] \n",
      "target_outputs_XOR: [0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "# Create input and target output data\n",
    "inputs = [[0, 0],\n",
    "          [1, 0],\n",
    "          [0, 1],\n",
    "          [1, 1]]\n",
    "print(f\"Input data: {inputs}\" )\n",
    "\n",
    "target_outputs_AND = [0, 0, 0, 1]\n",
    "print(f\"target_outputs_AND: {target_outputs_AND}\" )\n",
    "\n",
    "target_outputs_OR = [0, 1, 1, 1]\n",
    "print(f\"target_outputs_OR: {target_outputs_OR} \")\n",
    "\n",
    "target_outputs_XOR = [0, 1, 1, 0]\n",
    "print(f\"target_outputs_XOR: {target_outputs_XOR}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Implementing a class for a Perceptron classifier\n",
    "\n",
    "Now lets write a function to build and train a Perceptron.\n",
    "This is just an implementation of the algorithm above, except we are going to train one **step** or one **epoch** at a time.\n",
    "This allows us to see what the algorithm is doing more clearly.\n",
    "\n",
    "- A training **step** applies the algorithm to just one input example (A, B and C above).\n",
    "- An **epoch** repeats the training step for all input examples in the data (so in this case 4).\n",
    "\n",
    "First we define the learning rate and model.  \n",
    "**Run the next cell** to define the class we used in the lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class two_input_perceptron:\n",
    "    \"\"\" Simple implementation of perceptron with two inputs\"\"\"\n",
    "\n",
    "    def  __init__(self, learning_rate:float=0.1):\n",
    "        \"\"\" create a perceptron initialised with random weights\"\"\"\n",
    "        self.weight1 = np.random.rand()\n",
    "        self.weight2 = np.random.rand()\n",
    "        self.bias_weight = np.random.rand()\n",
    "        self.learning_rate = learning_rate\n",
    "        print(f\"Perceptron created with initial random weights: {self.__dict__}\")\n",
    "\n",
    "\n",
    "    def fit(self, data:np.ndarray, labels:np.array, max_epochs:int=50):\n",
    "        \"\"\" fits the perceptron weights to the supplied data \"\"\"\n",
    "\n",
    "        # loop for a number of epochs\n",
    "        for epoch in range(max_epochs):\n",
    "            errors_this_epoch = 0\n",
    "\n",
    "            # go through each training example in turn\n",
    "            for example in range(len(data)):\n",
    "\n",
    "                input1 = data[example][0]\n",
    "                input2 = data[example][1]\n",
    "                target = labels[example]\n",
    "\n",
    "                # calculate the prediction and error\n",
    "                prediction = self.predict(input1, input2)\n",
    "                error = target - prediction\n",
    "\n",
    "                # update the weights if there is an error\n",
    "                if error:\n",
    "                    errors_this_epoch += 1\n",
    "                    self.bias_weight += error * 1.0 * self.learning_rate # bias input is always +1\n",
    "                    self.weight1 += error * input1 * self.learning_rate\n",
    "                    self.weight2 += error * input2 * self.learning_rate\n",
    "\n",
    "                self.print_message(input1, input2, target, prediction)\n",
    "\n",
    "            # print message and decide whether to continue\n",
    "            if(errors_this_epoch > 0):\n",
    "                print(f\"Overall in epoch {epoch} there were {errors_this_epoch} errors\\n\")\n",
    "            else:\n",
    "                print(f\"Perceptron solved the learning problem in {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "\n",
    "    def predict(self, input1:int, input2:int) -> int:\n",
    "\n",
    "        # step 1 multiply each input by its weight and sum them\n",
    "        summed_input = input1*self.weight1 + input2*self.weight2 + 1.0*self.bias_weight\n",
    "\n",
    "        # step 2 compare sum to threshold (0) to decide output\n",
    "        if summed_input > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def print_message(self, input1:int, input2:int, target:int, prediction:int):\n",
    "        error = target - prediction\n",
    "        message = (f\"Input1: {input1} Input 2: {input2}, \"\n",
    "                            f\"target label {target}, \"\n",
    "                            f\"predicted label {prediction} \"\n",
    "                f\"so error = {error:2d}. \")\n",
    "        if not error:\n",
    "            message += \"So no update\"\n",
    "        else:\n",
    "            message += (f\"After updates: w1 {self.weight1:.4f}, \"\n",
    "                        f\"w2 {self.weight2:.4f} \"\n",
    "                        f\"biasweight {self.bias_weight:.4f}\")\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 3: Train the perceptron\n",
    "\n",
    "As it trains, in each epoch you will be told when it makes a prediction error, and what the updated weights are you should see output for the current inputs and target outputs,\n",
    "training step, epoch and total error for that epoch.  \n",
    "**Run the next cell** to **create** classifier model  and **fit** (train) it on the AND data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron created with initial random weights: {'weight1': 0.8194946916396705, 'weight2': 0.2349645079414422, 'bias_weight': 0.24156726542653328, 'learning_rate': 0.1}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.8195, w2 0.2350 biasweight 0.1416\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.7195, w2 0.2350 biasweight 0.0416\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.7195, w2 0.1350 biasweight -0.0584\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 0 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.6195, w2 0.1350 biasweight -0.1584\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 1 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.5195, w2 0.1350 biasweight -0.2584\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 2 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.4195, w2 0.1350 biasweight -0.3584\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 3 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.3195, w2 0.1350 biasweight -0.4584\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.4195, w2 0.2350 biasweight -0.3584\n",
      "Overall in epoch 4 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.3195, w2 0.2350 biasweight -0.4584\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 5 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 6 epochs\n"
     ]
    }
   ],
   "source": [
    "# Select the data and target outputs to use (AND, OR, XOR)\n",
    "target_outputs = target_outputs_AND\n",
    "\n",
    "# Create a perceptron and fit it to the data\n",
    "lr = 0.1\n",
    "my_perceptron = two_input_perceptron(learning_rate=lr)\n",
    "\n",
    "my_perceptron.fit(inputs, target_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1: Perceptrons - Logical Operators\n",
    "\n",
    "## Objective\n",
    "Familiarize yourself with the Perceptron algorithm, understand how it learns by updating weights, and experiment with different parameters and datasets.\n",
    "\n",
    "## Step-by-Step Solution\n",
    "\n",
    "### Compare the Pseudocode/Algorithm with the Code Implementation\n",
    "\n",
    "#### Pseudocode Steps:\n",
    "1. **Initialize weights**: Set weights to random values in range [-0.5, 0.5].\n",
    "2. **Set learning rate**: Choose a small value (usually < 0.5).\n",
    "3. **Training Loop (for each epoch and example):**\n",
    "   - **A. Calculate output (activation):** Compute weighted sum of inputs, apply threshold (if sum > 0, activation = 1, else 0).\n",
    "   - **B. Compute error:** Difference between target output and activation.\n",
    "   - **C. Update weights:** Adjust using the rule: `change in weight = error × input × learning rate`.\n",
    "4. **Repeat** until the error is 0 or for a fixed number of epochs.\n",
    "\n",
    "#### Code Implementation (`two_input_perceptron` Class)\n",
    "\n",
    "##### **Initialization (`__init__`):**\n",
    "- Weights (`weight1`, `weight2`, `bias_weight`) are initialized randomly using `np.random.rand()` (0 to 1 range, not [-0.5, 0.5], but close enough for simplicity).\n",
    "- Learning rate is set (default 0.1).\n",
    "\n",
    "##### **Fit Method (`fit`):**\n",
    "- Loops over epochs and training examples:\n",
    "  - Computes prediction using `predict` (sums weighted inputs + bias, thresholds at 0).\n",
    "  - Computes error as `target - prediction`.\n",
    "  - Updates weights only if there’s an error using:\n",
    "    ```\n",
    "    weight += error * input * learning_rate\n",
    "    ```\n",
    "  - Similarly, updates bias.\n",
    "\n",
    "##### **Prediction (`predict`):**\n",
    "- Matches pseudocode by summing weighted inputs and applying a step function (threshold at 0).\n",
    "\n",
    "### How It Learns\n",
    "The Perceptron adjusts weights based on errors to minimize the difference between predicted and target outputs. If the prediction is wrong (`error ≠ 0`), weights are adjusted in the direction that reduces error, scaled by the learning rate. This process continues until no errors occur or a maximum epochs limit is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Learning Rate\n",
    "\n",
    "We’ll modify the code to test different learning rates. The learning rate controls how much weights change per update—smaller values make learning slower but more stable, while larger values can speed up learning but risk instability (oscillations or divergence).\n",
    "\n",
    "### **Modified Code to Experiment with Learning Rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with learning rate = 0.01\n",
      "Perceptron created with initial random weights: {'weight1': -0.20375846993690983, 'weight2': 0.24664947522825675, 'bias_weight': -0.326202874993164, 'learning_rate': 0.01}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1938, w2 0.2566, biasweight -0.3162\n",
      "Overall in epoch 0 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1838, w2 0.2666, biasweight -0.3062\n",
      "Overall in epoch 1 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1738, w2 0.2766, biasweight -0.2962\n",
      "Overall in epoch 2 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1638, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 3 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1638, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1538, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 4 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1538, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1438, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 5 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1438, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1338, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 6 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1338, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1238, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 7 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1238, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1138, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 8 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1138, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1038, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 9 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1038, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0938, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 10 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0938, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0838, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 11 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0838, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0738, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 12 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0738, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0638, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 13 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0638, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0538, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 14 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0538, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0438, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 15 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0438, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0338, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 16 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0338, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0238, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 17 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0238, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0138, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 18 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0138, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0038, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 19 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0038, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.0062, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 20 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.0062, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.0162, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 21 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.0162, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.0262, w2 0.2866, biasweight -0.2862\n",
      "Overall in epoch 22 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.0262, w2 0.2766, biasweight -0.2962\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 23 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 24 epochs\n",
      "\n",
      "Testing with learning rate = 0.1\n",
      "Perceptron created with initial random weights: {'weight1': 0.27223360725587187, 'weight2': -0.058089240708533496, 'bias_weight': 0.11108761219237884, 'learning_rate': 0.1}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.2722, w2 -0.0581, biasweight 0.0111\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.1722, w2 -0.0581, biasweight -0.0889\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 0 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.0722, w2 -0.0581, biasweight -0.1889\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.1722, w2 0.0419, biasweight -0.0889\n",
      "Overall in epoch 1 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.0722, w2 0.0419, biasweight -0.1889\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.1722, w2 0.1419, biasweight -0.0889\n",
      "Overall in epoch 2 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.0722, w2 0.1419, biasweight -0.1889\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 3 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 4 epochs\n",
      "\n",
      "Testing with learning rate = 0.5\n",
      "Perceptron created with initial random weights: {'weight1': 0.308236862118048, 'weight2': -0.29373571510637697, 'bias_weight': -0.22126999867379837, 'learning_rate': 0.5}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1918, w2 -0.2937, biasweight -0.7213\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.3082, w2 0.2063, biasweight -0.2213\n",
      "Overall in epoch 0 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1918, w2 0.2063, biasweight -0.7213\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.3082, w2 0.7063, biasweight -0.2213\n",
      "Overall in epoch 1 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1918, w2 0.7063, biasweight -0.7213\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.3082, w2 1.2063, biasweight -0.2213\n",
      "Overall in epoch 2 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1918, w2 1.2063, biasweight -0.7213\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1918, w2 0.7063, biasweight -1.2213\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.3082, w2 1.2063, biasweight -0.7213\n",
      "Overall in epoch 3 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.3082, w2 0.7063, biasweight -1.2213\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.8082, w2 1.2063, biasweight -0.7213\n",
      "Overall in epoch 4 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.3082, w2 1.2063, biasweight -1.2213\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 5 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 6 epochs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "# Create input and target output data\n",
    "inputs = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "target_outputs_AND = [0, 0, 0, 1]\n",
    "target_outputs_OR = [0, 1, 1, 1]\n",
    "target_outputs_XOR = [0, 1, 1, 0]\n",
    "\n",
    "class two_input_perceptron:\n",
    "    \"\"\" Simple implementation of perceptron with two inputs\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate:float=0.1):\n",
    "        \"\"\" create a perceptron initialised with random weights\"\"\"\n",
    "        self.weight1 = np.random.rand() - 0.5  # Shift to [-0.5, 0.5] range\n",
    "        self.weight2 = np.random.rand() - 0.5\n",
    "        self.bias_weight = np.random.rand() - 0.5\n",
    "        self.learning_rate = learning_rate\n",
    "        print(f\"Perceptron created with initial random weights: {self.__dict__}\")\n",
    "\n",
    "    def fit(self, data:np.ndarray, labels:np.array, max_epochs:int=50):\n",
    "        \"\"\" fits the perceptron weights to the supplied data \"\"\"\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            errors_this_epoch = 0\n",
    "\n",
    "            for example in range(len(data)):\n",
    "\n",
    "                input1 = data[example][0]\n",
    "                input2 = data[example][1]\n",
    "                target = labels[example]\n",
    "\n",
    "                prediction = self.predict(input1, input2)\n",
    "                error = target - prediction\n",
    "\n",
    "                if error:\n",
    "                    errors_this_epoch += 1\n",
    "                    self.bias_weight += error * 1.0 * self.learning_rate\n",
    "                    self.weight1 += error * input1 * self.learning_rate\n",
    "                    self.weight2 += error * input2 * self.learning_rate\n",
    "\n",
    "                self.print_message(input1, input2, target, prediction)\n",
    "\n",
    "            if(errors_this_epoch > 0):\n",
    "                print(f\"Overall in epoch {epoch} there were {errors_this_epoch} errors\\n\")\n",
    "            else:\n",
    "                print(f\"Perceptron solved the learning problem in {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "    def predict(self, input1:int, input2:int) -> int:\n",
    "        summed_input = input1*self.weight1 + input2*self.weight2 + 1.0*self.bias_weight\n",
    "        return 1 if summed_input > 0 else 0\n",
    "\n",
    "    def print_message(self, input1:int, input2:int, target:int, prediction:int):\n",
    "        error = target - prediction\n",
    "        message = (f\"Input1: {input1} Input 2: {input2}, \"\n",
    "                   f\"target label {target}, \"\n",
    "                   f\"predicted label {prediction} \"\n",
    "                   f\"so error = {error:2d}. \")\n",
    "        if not error:\n",
    "            message += \"So no update\"\n",
    "        else:\n",
    "            message += (f\"After updates: w1 {self.weight1:.4f}, \"\n",
    "                        f\"w2 {self.weight2:.4f}, \"\n",
    "                        f\"biasweight {self.bias_weight:.4f}\")\n",
    "        print(message)\n",
    "\n",
    "# Experiment with different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5]  # Test these values\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting with learning rate = {lr}\")\n",
    "    target_outputs = target_outputs_AND  # Start with AND\n",
    "    my_perceptron = two_input_perceptron(learning_rate=lr)\n",
    "    my_perceptron.fit(inputs, target_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations on Learning Rate\n",
    "\n",
    "- **Low learning rate (0.01)**: Convergence is slower, requiring more epochs for the model to learn, but it remains stable throughout the training process.\n",
    "- **Medium learning rate (0.1)**: Provides a balanced approach—convergence is faster compared to a low learning rate, with good stability and fewer epochs needed.\n",
    "- **High learning rate (0.5)**: Convergence occurs quickly, but the model may experience oscillation or fail to converge properly if the learning rate is too high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Target Outputs and Make Predictions\n",
    "\n",
    "We will switch between `target_outputs_AND`, `target_outputs_OR`, and `target_outputs_XOR`.  \n",
    "For each case, we’ll make predictions before running the model and document our reasoning.\n",
    "\n",
    "## Modified Code with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing AND\n",
      "Prediction before running:\n",
      "- I predict the output will vary slightly each run due to random weight initialization, but it should converge because AND is linearly separable. The Perceptron should solve it in a few epochs.\n",
      "Perceptron created with initial random weights: {'weight1': -0.16127982231071825, 'weight2': 0.24168399256505224, 'bias_weight': 0.18750872331913482, 'learning_rate': 0.1}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1613, w2 0.2417, biasweight 0.0875\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1613, w2 0.1417, biasweight -0.0125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0613, w2 0.2417, biasweight 0.0875\n",
      "Overall in epoch 0 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0613, w2 0.2417, biasweight -0.0125\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0613, w2 0.1417, biasweight -0.1125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.0387, w2 0.2417, biasweight -0.0125\n",
      "Overall in epoch 1 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0613, w2 0.2417, biasweight -0.1125\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0613, w2 0.1417, biasweight -0.2125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.0387, w2 0.2417, biasweight -0.1125\n",
      "Overall in epoch 2 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.0387, w2 0.1417, biasweight -0.2125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.1387, w2 0.2417, biasweight -0.1125\n",
      "Overall in epoch 3 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.0387, w2 0.2417, biasweight -0.2125\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.0387, w2 0.1417, biasweight -0.3125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.1387, w2 0.2417, biasweight -0.2125\n",
      "Overall in epoch 4 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 0.1387, w2 0.1417, biasweight -0.3125\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 0.2387, w2 0.2417, biasweight -0.2125\n",
      "Overall in epoch 5 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.1387, w2 0.2417, biasweight -0.3125\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 6 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 7 epochs\n",
      "\n",
      "Testing OR\n",
      "Prediction before running:\n",
      "- I predict similar variability but convergence, as OR is also linearly separable. It should solve it quickly.\n",
      "Perceptron created with initial random weights: {'weight1': -0.1874464178849311, 'weight2': 0.2715838894293461, 'bias_weight': -0.04289611472121113, 'learning_rate': 0.1}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.0874, w2 0.2716, biasweight 0.0571\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 0 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0874, w2 0.2716, biasweight -0.0429\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 0.0126, w2 0.2716, biasweight 0.0571\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 1 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.0126, w2 0.2716, biasweight -0.0429\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 0.1126, w2 0.2716, biasweight 0.0571\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 2 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 0.1126, w2 0.2716, biasweight -0.0429\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Overall in epoch 3 there were 1 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 0 so error =  0. So no update\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Perceptron solved the learning problem in 4 epochs\n",
      "\n",
      "Testing XOR\n",
      "Prediction before running:\n",
      "- I predict it won’t solve XOR because the data points (0,1) and (1,0) need to be on opposite sides of the decision boundary, but a straight line can’t separate them (not linearly separable).\n",
      "Perceptron created with initial random weights: {'weight1': -0.0037367764850164686, 'weight2': -0.20403246187653323, 'bias_weight': 0.4527617545470053, 'learning_rate': 0.1}\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.0037, w2 -0.2040, biasweight 0.3528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1037, w2 -0.3040, biasweight 0.2528\n",
      "Overall in epoch 0 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1037, w2 -0.3040, biasweight 0.1528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 0 so error =  0. So no update\n",
      "Overall in epoch 1 there were 2 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 1 so error =  0. So no update\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 2 there were 3 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 3 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 4 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 5 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 6 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 7 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 8 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 9 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 10 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 11 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 12 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 13 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 14 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 15 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 16 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 17 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 18 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 19 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 20 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 21 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 22 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 23 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 24 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 25 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 26 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 27 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 28 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 29 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 30 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 31 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 32 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 33 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 34 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 35 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 36 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 37 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 38 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 39 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 40 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 41 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 42 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 43 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 44 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 45 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 46 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 47 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 48 there were 4 errors\n",
      "\n",
      "Input1: 0 Input 2: 0, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.0528\n",
      "Input1: 1 Input 2: 0, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.2040, biasweight 0.1528\n",
      "Input1: 0 Input 2: 1, target label 1, predicted label 0 so error =  1. After updates: w1 -0.1037, w2 -0.1040, biasweight 0.2528\n",
      "Input1: 1 Input 2: 1, target label 0, predicted label 1 so error = -1. After updates: w1 -0.2037, w2 -0.2040, biasweight 0.1528\n",
      "Overall in epoch 49 there were 4 errors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define all target outputs\n",
    "inputs = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "target_outputs_AND = [0, 0, 0, 1]\n",
    "target_outputs_OR = [0, 1, 1, 1]\n",
    "target_outputs_XOR = [0, 1, 1, 0]\n",
    "\n",
    "# Function to test and predict\n",
    "def test_perceptron(targets, name):\n",
    "    print(f\"\\nTesting {name}\")\n",
    "    # Prediction before running:\n",
    "    print(\"Prediction before running:\")\n",
    "    if name == \"AND\":\n",
    "        print(\"- I predict the output will vary slightly each run due to random weight initialization, but it should converge because AND is linearly separable. The Perceptron should solve it in a few epochs.\")\n",
    "    elif name == \"OR\":\n",
    "        print(\"- I predict similar variability but convergence, as OR is also linearly separable. It should solve it quickly.\")\n",
    "    elif name == \"XOR\":\n",
    "        print(\"- I predict it won’t solve XOR because the data points (0,1) and (1,0) need to be on opposite sides of the decision boundary, but a straight line can’t separate them (not linearly separable).\")\n",
    "\n",
    "    my_perceptron = two_input_perceptron(learning_rate=0.1)\n",
    "    my_perceptron.fit(inputs, targets)\n",
    "\n",
    "# Run tests\n",
    "test_perceptron(target_outputs_AND, \"AND\")\n",
    "test_perceptron(target_outputs_OR, \"OR\")\n",
    "test_perceptron(target_outputs_XOR, \"XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Predictions and Reasoning (Before Running)\n",
    "\n",
    "### 1. AND Gate\n",
    "- **Will it be the same each run?**  \n",
    "  No, because weights start randomly, so the exact path and number of epochs may differ. However, the final solution should be similar, as the weights adjust to separate (1,1) from others.\n",
    "- **Will it solve the problem?**  \n",
    "  Yes, because AND is linearly separable (only (1,1) should output 1, while the others should output 0). A straight line can separate the points.\n",
    "\n",
    "### 2. OR Gate\n",
    "- **Will it be the same each run?**  \n",
    "  No, due to random initialization, but the model should converge to a solution.\n",
    "- **Will it solve the problem?**  \n",
    "  Yes, OR is linearly separable (only (0,0) should output 0, while the others should output 1). A line can separate (0,0) from the rest.\n",
    "\n",
    "### 3. XOR Gate\n",
    "- **Will it be the same each run?**  \n",
    "  No, but the outcome will likely be failure each time.\n",
    "- **Will it solve the problem?**  \n",
    "  No, because XOR is not linearly separable. The points (0,1) and (1,0) should be 1, while (0,0) and (1,1) should be 0. No straight line can separate these classes in 2D space.\n",
    "\n",
    "---\n",
    "\n",
    "## After Running: Comparison to Predictions\n",
    "\n",
    "### 1. AND Gate\n",
    "- **Output Summary:**\n",
    "  - Converged in 7 epochs.\n",
    "  - Initial weights: `w1 = -0.1613, w2 = 0.2417, bias_weight = 0.1875`.\n",
    "  - Final weights: `w1 = 0.1387, w2 = 0.2417, bias_weight = -0.3125`.\n",
    "  - Errors reduced from 3 in epoch 0 to 0 by epoch 7.\n",
    "  - Example progression:\n",
    "    - Epoch 0: 3 errors (e.g., misclassified (0,0) as 1, (1,1) as 0).\n",
    "    - Epoch 6: 1 error (misclassified (1,0) as 1).\n",
    "    - Epoch 7: 0 errors (all inputs correctly classified).\n",
    "\n",
    "- **Comparison to Predictions:**\n",
    "  - **Variability:** As predicted, the output varied due to random weight initialization. The exact number of epochs (7) and weights differ across runs, but the solution (correct classification) is consistent, aligning with \"similar final solution.\"\n",
    "  - **Convergence:** Solved the problem, as predicted, due to AND's linear separability. The final weights define a boundary separating (1,1) (output 1) from others (output 0).\n",
    "  - **Analysis:** Took 7 epochs, slightly more than \"a few,\" but reasonable. Weights adjusted iteratively to refine the boundary.\n",
    "\n",
    "### 2. OR Gate\n",
    "- **Output Summary:**\n",
    "  - Converged in 4 epochs.\n",
    "  - Initial weights: `w1 = -0.1874, w2 = 0.2716, bias_weight = -0.0429`.\n",
    "  - Final weights: `w1 = 0.1126, w2 = 0.2716, bias_weight = -0.0429`.\n",
    "  - Errors reduced from 1 in epoch 0 to 0 by epoch 4.\n",
    "  - Example progression:\n",
    "    - Epoch 0: 1 error (misclassified (1,0) as 0).\n",
    "    - Epoch 3: 1 error (misclassified (0,0) as 1).\n",
    "    - Epoch 4: 0 errors (all inputs correctly classified).\n",
    "\n",
    "- **Comparison to Predictions:**\n",
    "  - **Variability:** As predicted, variability occurred due to random initialization. Epochs (4) and weights vary, but convergence is consistent, matching \"similar variability but convergence.\"\n",
    "  - **Convergence:** Solved quickly (4 epochs), as predicted, due to OR's linear separability. The boundary isolates (0,0) (output 0) from others (output 1).\n",
    "  - **Analysis:** Converged faster than AND, aligning with \"quickly.\" Fewer initial errors reflect simpler separation.\n",
    "\n",
    "### 3. XOR Gate\n",
    "- **Output Summary:**\n",
    "  - Did not converge after 50 epochs (max limit).\n",
    "  - Initial weights: `w1 = -0.0037, w2 = -0.2040, bias_weight = 0.4528`.\n",
    "  - Final weights: `w1 = -0.2037, w2 = -0.2040, bias_weight = 0.1528`.\n",
    "  - Errors fluctuated between 2 and 4, never reaching 0.\n",
    "  - Example progression:\n",
    "    - Epoch 0: 2 errors (e.g., misclassified (0,0) as 1, (1,1) as 1).\n",
    "    - Epoch 3 onwards: 4 errors repeatedly (e.g., (0,0) as 1, (1,0) as 0, (0,1) as 0, (1,1) as 1).\n",
    "    - Pattern: Oscillates without solution.\n",
    "\n",
    "- **Comparison to Predictions:**\n",
    "  - **Variability:** As predicted, output varies (error patterns differ), but failure is consistent, matching \"failure each time.\"\n",
    "  - **Convergence:** Did not solve, as predicted, due to XOR's non-linear separability. Persistent errors confirm no single line can separate the classes.\n",
    "  - **Analysis:** Oscillation (e.g., bias_weight toggling) validates the prediction of unsolvability.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Comparison Table\n",
    "\n",
    "| **Gate** | **Predicted Variability** | **Actual Variability** | **Predicted Convergence** | **Actual Convergence** | **Notes** |\n",
    "|----------|---------------------------|-------------------------|----------------------------|-------------------------|-----------|\n",
    "| **AND**  | No (random weights)       | Yes (7 epochs, weights vary) | Yes (linearly separable)   | Yes (7 epochs)          | Matches; slightly more epochs than expected. |\n",
    "| **OR**   | No (random weights)       | Yes (4 epochs, weights vary) | Yes (linearly separable)   | Yes (4 epochs)          | Matches; fast as predicted. |\n",
    "| **XOR**  | No (failure each time)    | Yes (consistent failure)     | No (not linearly separable)| No (50 epochs, no solution) | Perfect match; confirms limitation. |\n",
    "\n",
    "---\n",
    "\n",
    "## Insights\n",
    "- **Variability:** Random initialization caused different paths, as predicted, but outcomes aligned with separability.\n",
    "- **Convergence:** Linearly separable problems (AND, OR) succeeded; XOR failed due to its non-linear nature.\n",
    "- **Learning Process:** Weights adjusted based on errors, refining boundaries for AND/OR, but oscillating in XOR.\n",
    "\n",
    "### Additional Observations\n",
    "- **AND vs. OR:** AND took longer (7 vs. 4 epochs) due to more \"0\" points, reflecting complexity despite separability.\n",
    "- **XOR Oscillation:** Repeating 4-error pattern shows the Perceptron cycling, unable to resolve conflicts.\n",
    "- **Learning Rate:** Fixed at 0.1; worked for AND/OR but may exacerbate XOR’s oscillation.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "The predictions were accurate, correctly anticipating variability and convergence based on linear separability. The results highlight the Perceptron's strengths (solving AND/OR) and limitations (failing XOR), reinforcing the need for multi-layer networks for non-linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test your understanding\n",
    "**Run the cell below and answer the questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3c17c5262744818a83343f10aaea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), RadioButtons(options=(('biasweight', 0), ('weight1', 1), ('weight2', 2)), style=Descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae0231bcf26483b92b4bc28139392a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), RadioButtons(options=(('biasweight', 0), ('weight1', 1), ('weight2', 2)), style=Descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2164b4c220e148788702529c3738fa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), RadioButtons(options=(('yes', 0), ('no', 1)), style=DescriptionStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551253e50ea4536a01b7e5ec21762b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), RadioButtons(options=(('it is increased', 0), ('it is decreased', 1)), style=Descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba13d3db8b34c89b3322d89f8b400a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), RadioButtons(options=(('it is increased', 0), ('it is decreased', 1)), style=Descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import workbook7_mcq\n",
    "display(workbook7_mcq.Q1)\n",
    "\n",
    "display(workbook7_mcq.Q2)\n",
    "\n",
    "display(workbook7_mcq.Q3)\n",
    "\n",
    "display(workbook7_mcq.Q4)\n",
    "\n",
    "display(workbook7_mcq.Q5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 2: Perceptrons learn Straight Decision Boundaries!\n",
    "<img src=\"figures/straightLine.png\" width=\"300\" style=\"float:right\">\n",
    "\n",
    "To give you an intuition for what the Perceptron is doing, consider the equation for a straight line:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "*a*and *c* are coefficients just like the learned weights and bias in the Perceptron.\n",
    "\n",
    "Now lets think about when the perceptron's behaviour (output) changes as the inputs vary.\n",
    "- We know that the output depends on whether the sum of the weighted inputs is greater than zero (output 1) or not (output 0).\n",
    "- But if we are using the perceptron to make predictions,   \n",
    "  then saying that *the behaviour changes when ...*    \n",
    "  is the same as saying: *the decision boundary is when ...*\n",
    "  \n",
    "- In other words **the decision boundary for a perceptron** is when $y =0$ where      \n",
    "$y = input1 \\times weight1 \\;\\; + input2 \\times weight2\\;\\; + \\;\\;bias\\_weight$  \n",
    "\n",
    "\n",
    "Setting $y = 0$ and re-arranging the equation in terms of the two inputs gives:\n",
    "\n",
    "$input2 = - \\frac{weight1}{weight2} \\times input1 -  \\frac{bias\\_weight}{weight2}$\n",
    "\n",
    "Which is the same form as the equation for a straight line where:\n",
    "- the ratio of the bias weight to weight2 defines the intercept  \n",
    "  i.e., the critical value of input2 when input1 = 0\n",
    "- the ratio of weights 1 and 2 defines the slope/gradient of the line (a)  \n",
    "  i.e., how much the critical value of input2 changes each time input1 changes by +1 \n",
    "\n",
    "So for any given value of input1, we can use this equation to tell us the critical value of input2\n",
    "- above that the output is 1,  below that, the output is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 2: Interactively changing weights to mimic automatic learning</h2>\n",
    "Run the code cell below to create an interactive widget that allows you to manually adjust the weights, and see how the decision boundary moves.<br>\n",
    "\n",
    "<b>Note:</b>You don't need to understand the python in the first cell- it sets up the sliders, the radio buttons and the plot widget.\n",
    "\n",
    "<b>Note:</b> This widget will not work in VScode, you need to run it in Jupyterlab (in browser).\n",
    "\n",
    "<ul>\n",
    "    <li> When you click to show different functions, the colour (target) of the dots at (0,0),(0,1),(1,0) and (1,1) change.</li>\n",
    "    <li> The sliders let you manually control the values of the perceptron weights.</li>\n",
    "    <li> The red line shows the decision boundary calculated from the weight values.</li>\n",
    "    <li> When the weights are correct (i.e. the perceptron will correctly predict) that function:<br>\n",
    "            red dots should be 0 (below the line), and green dots should be 1 (above the line).</li>\n",
    "</ul>\n",
    "\n",
    "You do not need to remember the equation for the calculating the decision boundary. But you should try and understand how this decision boundary relates to the Perceptrons output (and why it can only be straight).\n",
    "<ol>\n",
    "    <li>Try different functions, and see if you can manually tweak the slider values so that the red line separates the red and green dots.</li>\n",
    "    <li>You might find it helpful to go around the four points (00, 10, 01, 11) in turn, and looking at whether they are on the wrong side of the line.</li>\n",
    "    <li>If so, moving the appropriate sliders a little by hand, to mimic what the update mechanism does automatically.</li>\n",
    "    <li> Finally, answer the questions in the second cell.</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "**Run the cell below** and **experiment** to observe and understand the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561a20b780144035afeb484d742f243b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-0.5, description='w1', max=1.0, min=-1.0), FloatSlider(value=0.5, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import matplotlib.pyplot as plt\n",
    "import workbook7_mcq\n",
    "%matplotlib inline\n",
    "\n",
    "# Create sliders for the weights and bias\n",
    "weight1 = widgets.FloatSlider(value=-0.5, min=-1, max=1)\n",
    "weight2 = widgets.FloatSlider(value=0.5, min=-1, max=1)\n",
    "biasweight = widgets.FloatSlider(value=-0.5, min=-1, max=1)\n",
    "funcToModel = widgets.RadioButtons(options=['OR','AND','XOR'])\n",
    "# Create the interactive plot\n",
    "output = interact(workbook7_mcq.showPerceptron, w1=weight1, w2=weight2, bias=biasweight, func=funcToModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8d5efa31d7429fb6a89369001c6ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(outputs=({'name': 'stdout', 'text': 'Is there only one set of weights that would output …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905e22418ed94a28a3e791b4623dc447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(outputs=({'name': 'stdout', 'text': 'if a perceptron has learned to correctly predict re…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(workbook7_mcq.Q6)\n",
    "display(workbook7_mcq.Q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 3: Learning data with real-value (continuous) features\n",
    "\n",
    "Truth table data and logical functions are a good way to learn the Perceptron algorithm but the data isn't very realistic.\n",
    "\n",
    "Most problems are much more complex and cannot be represented with binary data or solved with only 4 training examples.  \n",
    "We were also only training for one **step** (one input example) or one **epoch** (all input examples) at a time, so that we\n",
    "could see what the algorithm was doing.\n",
    "\n",
    "In supervised learning, generally we want to train for a fixed number of epochs, or until there is no improvement in\n",
    "the error on the training data.   \n",
    "Once training is finished we apply the model (trained weights) to some test data and\n",
    "measure its performance.   \n",
    "This gives us an indication of how well it would perform on new data it has not 'seen' before.\n",
    "\n",
    "Next we will train and then test a Perceptron on a larger, real numbered dataset so that we can see the process of \n",
    "applying machine learning in practice. \n",
    "- We'll use the Iris one, as you should be familiar with it by now.\n",
    "\n",
    "- **The difference** is that perceptrons only handle two classes \n",
    "  - so to start with we will just make a binary setosa/not-setosa classifier.\n",
    "\n",
    "As before, we will first import some python modules and then the load the iris data\n",
    "- The other call in the first cell splits the data created into train and test sets. \n",
    "\n",
    "- you **don't** have to understand how the visualisation code works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Iris data\n",
    "\n",
    "**Run the cell below** to load the data, convert it into a 2 class (binary problem) and split into training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the data\n",
    "irisX, irisy = load_iris(return_X_y=True)\n",
    "\n",
    "# Convert into a binary classification problem\n",
    "flower_class = 0  # setosa (could be 1 for versicolor, or 2 for virginica)\n",
    "\n",
    "# Using numpy *where* function - the equivalent of an if, but applies to a whole array\n",
    "class_labels = np.where(irisy==flower_class, 1, 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(irisX, class_labels, test_size=0.33, stratify=irisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implementing n input perceptron\n",
    "\n",
    "This is the same algorithm as before, the only difference is it will work with variable (n) numbers of input features instead of 'hard coding' the number of weights, because we need one weight per input.\n",
    "\n",
    "The class below extends the two-input perceptron class from above, so that it:\n",
    "\n",
    "1. **Is more generic** - and  can cope with any number of inputs.   \n",
    "   - To achieve that we move the weight initialisation from the constructor to the fit() function.\n",
    "   - Use a numpy array of weights rather than holding them in a fixed set of variables.  \n",
    "   - This lets it  deal with data with any number of features by querying the data's *shape* attribute.<br><br>\n",
    "\n",
    "3. Has a **fit()** method that uses a vector of weights rather than making assumptions about data shape.\n",
    "   - This also means it will match the sklearn models way of doing things.\n",
    "   - Training will stop:\n",
    "     - when  the whole training set is presented (one epoch) with no prediction errors, or  \n",
    "     - after a fixed number of epochs have been run - because we can't assume 100% accuracy is achievable!<br><br>\n",
    "\n",
    "3. Implements a **predict()** method that  takes a set of items to predict as a parameter.\n",
    "   - This method can be used  to estimate the performance of our model on a held back test set.\n",
    "   - It just loops over the cases calling a method predict_one().<br><br>\n",
    "\n",
    "4. Implements a method **predict_one()** that presents the  set of feature values corresponding to one new item/example to the network and returns the network's output value.\n",
    "\n",
    "**Run the cell below to define this class.** Ask your peers or a tutor if you do not understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class n_input_perceptron:\n",
    "    def  __init__(self, learning_rate:float=0.1, debug=True):\n",
    "        # The only weight we set in the constructor is a random initial  bias weight\n",
    "        self.bias_weight = random()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "\n",
    "    def fit(self, data:np.ndarray, labels:np.array, max_epochs:int=50):\n",
    "        \"\"\" fits the perceptron weights to the supplied data\"\"\"\n",
    "\n",
    "        # Find the number of input features in the data\n",
    "        self.num_inputs = data.shape[1]\n",
    "        # Then create that many weights and randomise them\n",
    "        self.weights = np.random.rand(self.num_inputs)\n",
    "\n",
    "        # loop for a number of epochs if needed\n",
    "        for epoch in range(max_epochs):\n",
    "            errors_this_epoch = 0\n",
    "\n",
    "            # loop through each training example in turn\n",
    "            for example in range(len(data)):\n",
    "\n",
    "                target = labels[example]\n",
    "\n",
    "                # calculate the prediction and error\n",
    "                prediction = self.predict_one(data[example])\n",
    "                error = target - prediction\n",
    "\n",
    "                # update the weights if there is an error\n",
    "                if error:\n",
    "                    errors_this_epoch += 1\n",
    "\n",
    "                    self.bias_weight += error * 1.0   *self.learning_rate  # bias input is always +1\n",
    "                    for position in range(self.num_inputs):\n",
    "                        self.weights[position] += error * data[example][position] * self.learning_rate\n",
    "\n",
    "            # print message and decide whether to continue\n",
    "            if(errors_this_epoch > 0):\n",
    "                print(f\"Epoch {epoch+1} there were {errors_this_epoch} errors\\n\")\n",
    "            else:\n",
    "                print(f\"Perceptron solved the learning problem in {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "        # we've finished!\n",
    "        self.is_fitted_=True\n",
    "\n",
    "    # this predict method is changed to take an array of inputs instead of just two\n",
    "    def predict_one(self, input_values) -> int:\n",
    "\n",
    "        summed_input =  self.bias_weight  * 1.0  # since bias always has the input value 1.0\n",
    "        for i in range(self.num_inputs):\n",
    "            summed_input += input_values[i] * self.weights[i]\n",
    "\n",
    "        # Threshold the sum to get the output prediction\n",
    "        return 1 if summed_input > 0 else 0\n",
    "\n",
    "\n",
    "    # The new predict() method will now accept a set of examples to make predictions for\n",
    "    # it just runs a loop repeatedly calling a function to predict for one case\n",
    "    def predict(self, data):\n",
    "\n",
    "        # ask the data how many rows it has\n",
    "        num_to_predict = data.shape[0]\n",
    "\n",
    "        predictions = []\n",
    "        for new_case in range(num_to_predict):\n",
    "            predictions.append (self.predict_one(data[new_case]) )\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 3: Learning from  examples with  continuous features</h2>\n",
    "The aim of this activity is to illustrate how Perceptrons can be applied to a larger, more realistic, dataset with real numbered features.\n",
    "The following cell trains a perceptron on the binary version of Iris data:\n",
    "<ol>\n",
    "    <li>Try changing the <code>learning_rate</code> parameter, which controls how large the change in weights is, and see how that effects learning.</li>\n",
    "    <li>Try changing the <code>max_epochs</code> parameter, which controls how many iterations of model adaptation happen.</li>\n",
    "    <li>Explore what effect these have on how well the perceptron learns the training data. Remember to run the second cell a few times since the perceptron starts with a random model (set of weights).</li>\n",
    "</ol>\n",
    "\n",
    "Then run the following cell to see how well it performs on the 'unseen' test data.<br>\n",
    "<b>Note:</b>Now we just call <code>predict()</code> but we dont update the weights.\n",
    "</div>\n",
    "\n",
    "**Run the cells below** to **create** an instance of this class, **fit** it  and then **evaluate** on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 there were 12 errors\n",
      "\n",
      "Perceptron solved the learning problem in 2 epochs\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "lr = 0.05\n",
    "epochs = 50\n",
    "\n",
    "# Create a perceptron and fit it to the data\n",
    "my_perceptron = n_input_perceptron(learning_rate=0.05)\n",
    "my_perceptron.fit(train_X, train_y, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the unseen test data the perceptron made 50 correct predictions and 0 errors so the accuracy is 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = my_perceptron.predict(test_X)\n",
    "\n",
    "# Compare those to the true values to see how well the perceptron did\n",
    "correct = 0\n",
    "for example in range(test_X.shape[0]):\n",
    "    if (predictions[example] == test_y[example]) :\n",
    "        correct += 1\n",
    "incorrect = correct - test_X.shape[0]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print (f\"On the unseen test data the perceptron made {correct} correct predictions and {incorrect} errors so the accuracy is {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with learning_rate and max_epochs\n",
    "\n",
    "Now, we'll create and fit the Perceptron, then evaluate it on test data. We'll modify learning_rate and max_epochs to see their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing the Effects of `learning_rate` and `max_epochs` on Perceptron Performance\n",
    "\n",
    "In this experiment, we will observe the impact of different values for `learning_rate` and `max_epochs` on the Perceptron model's performance. We will also analyze how the random initialization of weights affects the model's behavior.\n",
    "\n",
    "## Effect of `learning_rate`\n",
    "\n",
    "The `learning_rate` controls the step size during weight updates. Here's how different values affect the training process:\n",
    "\n",
    "- **Smaller `learning_rate` (e.g., 0.01):**\n",
    "  - **Slower learning:** The model updates weights more gradually.\n",
    "  - **More epochs needed to converge:** A smaller step size requires more iterations to minimize the error.\n",
    "  - **More stable learning:** There is less risk of overshooting the optimal solution, but this can lead to a longer training time and might not reach 0 errors if `max_epochs` is too low.\n",
    "\n",
    "- **Larger `learning_rate` (e.g., 0.1):**\n",
    "  - **Faster learning:** The model updates weights quickly, which speeds up convergence.\n",
    "  - **Fewer epochs required:** The model reaches the minimum error faster, but it might not achieve the best performance if it overshoots.\n",
    "  - **Potentially unstable:** The model might oscillate or fail to converge if the `learning_rate` is too high, especially when the dataset is complex.\n",
    "\n",
    "## Effect of `max_epochs`\n",
    "\n",
    "The `max_epochs` controls the number of iterations the model will run. Here's how different values affect the model's training:\n",
    "\n",
    "- **Fewer `max_epochs` (e.g., 20):**\n",
    "  - **Insufficient time for convergence:** The model may not have enough chances to learn, especially with a small `learning_rate`. This could result in lower accuracy if the model hasn't fully optimized its weights.\n",
    "  \n",
    "- **More `max_epochs` (e.g., 100):**\n",
    "  - **More chances to reduce errors:** With more epochs, the model has more time to find the optimal weights, which can lead to a lower error.\n",
    "  - **Excessive epochs waste time:** If the model has already converged early, continuing to train beyond that point won't lead to significant improvement. This could waste computational resources without providing any benefits.\n",
    "\n",
    "## Random Initialization of Weights\n",
    "\n",
    "Since the Perceptron algorithm begins with random weight initialization, the starting weights can significantly affect the model's convergence speed and final performance. Each time the cell is run, the weights are initialized differently, which may lead to slight variations in the final accuracy.\n",
    "\n",
    "- **Run the experiment multiple times**: Observe how the initial random weights affect convergence speed. For instance, some initial weight configurations might cause the model to converge faster, while others might require more iterations.\n",
    "\n",
    "## Evaluate on Unseen Test Data\n",
    "\n",
    "After training the Perceptron, we use the `predict()` method to evaluate the model's performance on the test data. This step is crucial because it helps us understand how well the model generalizes to new, unseen data. The test accuracy is an indicator of how effectively the model has learned from the training data and whether it can apply that knowledge to make correct predictions on previously unseen examples.\n",
    "\n",
    "- **Accuracy Metric**: The accuracy score shows the proportion of correct predictions on the test data. A higher accuracy means that the Perceptron is generalizing well, while a lower accuracy suggests that the model might be overfitting or underfitting.\n",
    "\n",
    "By observing the effect of different `learning_rate` and `max_epochs` values, as well as the randomness in weight initialization, we can better understand how to tune these hyperparameters for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment: learning_rate = 0.01, max_epochs = 20\n",
      "Epoch 1 there were 67 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.01, max_epochs = 50\n",
      "Epoch 1 there were 67 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.01, max_epochs = 100\n",
      "Epoch 1 there were 67 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Epoch 51 there were 67 errors\n",
      "\n",
      "Epoch 52 there were 67 errors\n",
      "\n",
      "Epoch 53 there were 67 errors\n",
      "\n",
      "Epoch 54 there were 67 errors\n",
      "\n",
      "Epoch 55 there were 67 errors\n",
      "\n",
      "Epoch 56 there were 67 errors\n",
      "\n",
      "Epoch 57 there were 67 errors\n",
      "\n",
      "Epoch 58 there were 67 errors\n",
      "\n",
      "Epoch 59 there were 67 errors\n",
      "\n",
      "Epoch 60 there were 67 errors\n",
      "\n",
      "Epoch 61 there were 67 errors\n",
      "\n",
      "Epoch 62 there were 67 errors\n",
      "\n",
      "Epoch 63 there were 67 errors\n",
      "\n",
      "Epoch 64 there were 67 errors\n",
      "\n",
      "Epoch 65 there were 67 errors\n",
      "\n",
      "Epoch 66 there were 67 errors\n",
      "\n",
      "Epoch 67 there were 67 errors\n",
      "\n",
      "Epoch 68 there were 67 errors\n",
      "\n",
      "Epoch 69 there were 67 errors\n",
      "\n",
      "Epoch 70 there were 67 errors\n",
      "\n",
      "Epoch 71 there were 67 errors\n",
      "\n",
      "Epoch 72 there were 67 errors\n",
      "\n",
      "Epoch 73 there were 67 errors\n",
      "\n",
      "Epoch 74 there were 67 errors\n",
      "\n",
      "Epoch 75 there were 67 errors\n",
      "\n",
      "Epoch 76 there were 67 errors\n",
      "\n",
      "Epoch 77 there were 67 errors\n",
      "\n",
      "Epoch 78 there were 67 errors\n",
      "\n",
      "Epoch 79 there were 67 errors\n",
      "\n",
      "Epoch 80 there were 67 errors\n",
      "\n",
      "Epoch 81 there were 67 errors\n",
      "\n",
      "Epoch 82 there were 67 errors\n",
      "\n",
      "Epoch 83 there were 67 errors\n",
      "\n",
      "Epoch 84 there were 67 errors\n",
      "\n",
      "Epoch 85 there were 67 errors\n",
      "\n",
      "Epoch 86 there were 67 errors\n",
      "\n",
      "Epoch 87 there were 67 errors\n",
      "\n",
      "Epoch 88 there were 67 errors\n",
      "\n",
      "Epoch 89 there were 67 errors\n",
      "\n",
      "Epoch 90 there were 67 errors\n",
      "\n",
      "Epoch 91 there were 67 errors\n",
      "\n",
      "Epoch 92 there were 67 errors\n",
      "\n",
      "Epoch 93 there were 67 errors\n",
      "\n",
      "Epoch 94 there were 67 errors\n",
      "\n",
      "Epoch 95 there were 67 errors\n",
      "\n",
      "Epoch 96 there were 67 errors\n",
      "\n",
      "Epoch 97 there were 67 errors\n",
      "\n",
      "Epoch 98 there were 67 errors\n",
      "\n",
      "Epoch 99 there were 67 errors\n",
      "\n",
      "Epoch 100 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.05, max_epochs = 20\n",
      "Epoch 1 there were 65 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.05, max_epochs = 50\n",
      "Epoch 1 there were 64 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.05, max_epochs = 100\n",
      "Epoch 1 there were 64 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Epoch 51 there were 67 errors\n",
      "\n",
      "Epoch 52 there were 67 errors\n",
      "\n",
      "Epoch 53 there were 67 errors\n",
      "\n",
      "Epoch 54 there were 67 errors\n",
      "\n",
      "Epoch 55 there were 67 errors\n",
      "\n",
      "Epoch 56 there were 67 errors\n",
      "\n",
      "Epoch 57 there were 67 errors\n",
      "\n",
      "Epoch 58 there were 67 errors\n",
      "\n",
      "Epoch 59 there were 67 errors\n",
      "\n",
      "Epoch 60 there were 67 errors\n",
      "\n",
      "Epoch 61 there were 67 errors\n",
      "\n",
      "Epoch 62 there were 67 errors\n",
      "\n",
      "Epoch 63 there were 67 errors\n",
      "\n",
      "Epoch 64 there were 67 errors\n",
      "\n",
      "Epoch 65 there were 67 errors\n",
      "\n",
      "Epoch 66 there were 67 errors\n",
      "\n",
      "Epoch 67 there were 67 errors\n",
      "\n",
      "Epoch 68 there were 67 errors\n",
      "\n",
      "Epoch 69 there were 67 errors\n",
      "\n",
      "Epoch 70 there were 67 errors\n",
      "\n",
      "Epoch 71 there were 67 errors\n",
      "\n",
      "Epoch 72 there were 67 errors\n",
      "\n",
      "Epoch 73 there were 67 errors\n",
      "\n",
      "Epoch 74 there were 67 errors\n",
      "\n",
      "Epoch 75 there were 67 errors\n",
      "\n",
      "Epoch 76 there were 67 errors\n",
      "\n",
      "Epoch 77 there were 67 errors\n",
      "\n",
      "Epoch 78 there were 67 errors\n",
      "\n",
      "Epoch 79 there were 67 errors\n",
      "\n",
      "Epoch 80 there were 67 errors\n",
      "\n",
      "Epoch 81 there were 67 errors\n",
      "\n",
      "Epoch 82 there were 67 errors\n",
      "\n",
      "Epoch 83 there were 67 errors\n",
      "\n",
      "Epoch 84 there were 67 errors\n",
      "\n",
      "Epoch 85 there were 67 errors\n",
      "\n",
      "Epoch 86 there were 67 errors\n",
      "\n",
      "Epoch 87 there were 67 errors\n",
      "\n",
      "Epoch 88 there were 67 errors\n",
      "\n",
      "Epoch 89 there were 67 errors\n",
      "\n",
      "Epoch 90 there were 67 errors\n",
      "\n",
      "Epoch 91 there were 67 errors\n",
      "\n",
      "Epoch 92 there were 67 errors\n",
      "\n",
      "Epoch 93 there were 67 errors\n",
      "\n",
      "Epoch 94 there were 67 errors\n",
      "\n",
      "Epoch 95 there were 67 errors\n",
      "\n",
      "Epoch 96 there were 67 errors\n",
      "\n",
      "Epoch 97 there were 67 errors\n",
      "\n",
      "Epoch 98 there were 67 errors\n",
      "\n",
      "Epoch 99 there were 67 errors\n",
      "\n",
      "Epoch 100 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.1, max_epochs = 20\n",
      "Epoch 1 there were 64 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.1, max_epochs = 50\n",
      "Epoch 1 there were 65 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n",
      "\n",
      "Experiment: learning_rate = 0.1, max_epochs = 100\n",
      "Epoch 1 there were 65 errors\n",
      "\n",
      "Epoch 2 there were 67 errors\n",
      "\n",
      "Epoch 3 there were 67 errors\n",
      "\n",
      "Epoch 4 there were 67 errors\n",
      "\n",
      "Epoch 5 there were 67 errors\n",
      "\n",
      "Epoch 6 there were 67 errors\n",
      "\n",
      "Epoch 7 there were 67 errors\n",
      "\n",
      "Epoch 8 there were 67 errors\n",
      "\n",
      "Epoch 9 there were 67 errors\n",
      "\n",
      "Epoch 10 there were 67 errors\n",
      "\n",
      "Epoch 11 there were 67 errors\n",
      "\n",
      "Epoch 12 there were 67 errors\n",
      "\n",
      "Epoch 13 there were 67 errors\n",
      "\n",
      "Epoch 14 there were 67 errors\n",
      "\n",
      "Epoch 15 there were 67 errors\n",
      "\n",
      "Epoch 16 there were 67 errors\n",
      "\n",
      "Epoch 17 there were 67 errors\n",
      "\n",
      "Epoch 18 there were 67 errors\n",
      "\n",
      "Epoch 19 there were 67 errors\n",
      "\n",
      "Epoch 20 there were 67 errors\n",
      "\n",
      "Epoch 21 there were 67 errors\n",
      "\n",
      "Epoch 22 there were 67 errors\n",
      "\n",
      "Epoch 23 there were 67 errors\n",
      "\n",
      "Epoch 24 there were 67 errors\n",
      "\n",
      "Epoch 25 there were 67 errors\n",
      "\n",
      "Epoch 26 there were 67 errors\n",
      "\n",
      "Epoch 27 there were 67 errors\n",
      "\n",
      "Epoch 28 there were 67 errors\n",
      "\n",
      "Epoch 29 there were 67 errors\n",
      "\n",
      "Epoch 30 there were 67 errors\n",
      "\n",
      "Epoch 31 there were 67 errors\n",
      "\n",
      "Epoch 32 there were 67 errors\n",
      "\n",
      "Epoch 33 there were 67 errors\n",
      "\n",
      "Epoch 34 there were 67 errors\n",
      "\n",
      "Epoch 35 there were 67 errors\n",
      "\n",
      "Epoch 36 there were 67 errors\n",
      "\n",
      "Epoch 37 there were 67 errors\n",
      "\n",
      "Epoch 38 there were 67 errors\n",
      "\n",
      "Epoch 39 there were 67 errors\n",
      "\n",
      "Epoch 40 there were 67 errors\n",
      "\n",
      "Epoch 41 there were 67 errors\n",
      "\n",
      "Epoch 42 there were 67 errors\n",
      "\n",
      "Epoch 43 there were 67 errors\n",
      "\n",
      "Epoch 44 there were 67 errors\n",
      "\n",
      "Epoch 45 there were 67 errors\n",
      "\n",
      "Epoch 46 there were 67 errors\n",
      "\n",
      "Epoch 47 there were 67 errors\n",
      "\n",
      "Epoch 48 there were 67 errors\n",
      "\n",
      "Epoch 49 there were 67 errors\n",
      "\n",
      "Epoch 50 there were 67 errors\n",
      "\n",
      "Epoch 51 there were 67 errors\n",
      "\n",
      "Epoch 52 there were 67 errors\n",
      "\n",
      "Epoch 53 there were 67 errors\n",
      "\n",
      "Epoch 54 there were 67 errors\n",
      "\n",
      "Epoch 55 there were 67 errors\n",
      "\n",
      "Epoch 56 there were 67 errors\n",
      "\n",
      "Epoch 57 there were 67 errors\n",
      "\n",
      "Epoch 58 there were 67 errors\n",
      "\n",
      "Epoch 59 there were 67 errors\n",
      "\n",
      "Epoch 60 there were 67 errors\n",
      "\n",
      "Epoch 61 there were 67 errors\n",
      "\n",
      "Epoch 62 there were 67 errors\n",
      "\n",
      "Epoch 63 there were 67 errors\n",
      "\n",
      "Epoch 64 there were 67 errors\n",
      "\n",
      "Epoch 65 there were 67 errors\n",
      "\n",
      "Epoch 66 there were 67 errors\n",
      "\n",
      "Epoch 67 there were 67 errors\n",
      "\n",
      "Epoch 68 there were 67 errors\n",
      "\n",
      "Epoch 69 there were 67 errors\n",
      "\n",
      "Epoch 70 there were 67 errors\n",
      "\n",
      "Epoch 71 there were 67 errors\n",
      "\n",
      "Epoch 72 there were 67 errors\n",
      "\n",
      "Epoch 73 there were 67 errors\n",
      "\n",
      "Epoch 74 there were 67 errors\n",
      "\n",
      "Epoch 75 there were 67 errors\n",
      "\n",
      "Epoch 76 there were 67 errors\n",
      "\n",
      "Epoch 77 there were 67 errors\n",
      "\n",
      "Epoch 78 there were 67 errors\n",
      "\n",
      "Epoch 79 there were 67 errors\n",
      "\n",
      "Epoch 80 there were 67 errors\n",
      "\n",
      "Epoch 81 there were 67 errors\n",
      "\n",
      "Epoch 82 there were 67 errors\n",
      "\n",
      "Epoch 83 there were 67 errors\n",
      "\n",
      "Epoch 84 there were 67 errors\n",
      "\n",
      "Epoch 85 there were 67 errors\n",
      "\n",
      "Epoch 86 there were 67 errors\n",
      "\n",
      "Epoch 87 there were 67 errors\n",
      "\n",
      "Epoch 88 there were 67 errors\n",
      "\n",
      "Epoch 89 there were 67 errors\n",
      "\n",
      "Epoch 90 there were 67 errors\n",
      "\n",
      "Epoch 91 there were 67 errors\n",
      "\n",
      "Epoch 92 there were 67 errors\n",
      "\n",
      "Epoch 93 there were 67 errors\n",
      "\n",
      "Epoch 94 there were 67 errors\n",
      "\n",
      "Epoch 95 there were 67 errors\n",
      "\n",
      "Epoch 96 there were 67 errors\n",
      "\n",
      "Epoch 97 there were 67 errors\n",
      "\n",
      "Epoch 98 there were 67 errors\n",
      "\n",
      "Epoch 99 there were 67 errors\n",
      "\n",
      "Epoch 100 there were 67 errors\n",
      "\n",
      "Test Accuracy: 34.00%\n",
      "Correct predictions: 17, Total predictions: 50\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters to experiment with\n",
    "learning_rates = [0.01, 0.05, 0.1]  # Different learning rates to test\n",
    "max_epochs_list = [20, 50, 100]     # Different max epochs to test\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in max_epochs_list:\n",
    "        print(f\"\\nExperiment: learning_rate = {lr}, max_epochs = {epochs}\")\n",
    "\n",
    "        # Create a new perceptron instance\n",
    "        my_perceptron = n_input_perceptron(learning_rate=lr, debug=True)\n",
    "\n",
    "        # Fit the perceptron to the training data\n",
    "        my_perceptron.fit(train_X, train_y, max_epochs=epochs)\n",
    "\n",
    "        # Make predictions on test data\n",
    "        predictions = my_perceptron.predict(test_X)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        correct = sum(1 for i in range(len(predictions)) if predictions[i] == test_y[i])\n",
    "        total = len(test_y)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Correct predictions: {correct}, Total predictions: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "\n",
    "## Learning from data with more  features and classes\n",
    "<img src=\"figures/cascading.png\" style=\"float:right\">\n",
    "\n",
    "Since perceptrons can only make two-way distinctions for multi-class data (like Iris) we have a choice of options:\n",
    "\n",
    "1. (simplest) we create three classifiers - one to recognise each class. \n",
    "    - This requires a way of specifying how to combine their votes,\n",
    "    - and what to do if all three say \"not in class\".\n",
    "\n",
    "2. (slightly more complex) use a cascade approach (shown in image)\n",
    "    - first train a network to predict if a training item is setosa or not.\n",
    "    - then use the training items that are predicted 'not-setosa' to train a second perceptron that predicts versicolor-or virginica.\n",
    "\n",
    "In the next few cells we show how to create a cascading classifier.\n",
    "\n",
    "**Run the cell below** to load the data set and split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris has 150 samples and 4 features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Iris has 3 classes: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris data\n",
    "iris_data = load_iris(return_X_y=False)\n",
    "# Extract the data and labels, feature names, and label names\n",
    "irisX = iris_data.data\n",
    "irisy = iris_data.target\n",
    "feature_names = iris_data.feature_names\n",
    "label_names = iris_data.target_names\n",
    "\n",
    "print(f\"Iris has {irisX.shape[0]} samples and {irisX.shape[1]} features: {feature_names}\")\n",
    "print(f\"Iris has 3 classes: {label_names}\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up  version of the labels which just treat the data as setosa (0) or not (1), by setting the values 2 (virginica) to 1 and the same thing to make some versicolor labels.\n",
    "**Run the cell below** to make sets of labels for the different 1-vs-all sub-problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 original   labels [0 1 2 0 0 0 0 2 2 0 1 1 1 0 0 1 1 1 0 0 2 1 2 2 2]\n",
      "First 25 setosa     labels [1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0]\n",
      "First 25 versicolor labels [0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# make zeros arrays of right size then loop through putting in 1s for the appropriate classes\n",
    "versicolor_train_y = np.zeros(len(train_y), dtype=int)\n",
    "setosa_train_y = np.zeros(len(train_y), dtype=int)\n",
    "for i in range(len(train_y)):\n",
    "    if train_y[i] == 0:\n",
    "        setosa_train_y[i] = 1\n",
    "    if train_y[i] == 1:\n",
    "        versicolor_train_y[i] = 1\n",
    "\n",
    "setosa_test_y = np.zeros(len(test_y), dtype=int)\n",
    "versicolor_test_y = np.zeros(len(test_y), dtype=int)\n",
    "for i in range(len(test_y)):\n",
    "    if test_y[i] == 0:\n",
    "        setosa_test_y[i] = 1\n",
    "    if test_y[i] == 1:\n",
    "        versicolor_test_y[i] = 1\n",
    "\n",
    "\n",
    "print(f\"First 25 original   labels {test_y[:25]}\")\n",
    "print(f\"First 25 setosa     labels {setosa_test_y[:25]}\")\n",
    "print(f\"First 25 versicolor labels {versicolor_test_y[:25]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Train a perceptron for the setosa : not setosa task\n",
    "**Run the cell below** to\n",
    "- train a perceptron to do the *setosa*:*not-setosa* recognition task.  \n",
    "-  make predictions  and get the id's of the training items classified as *not-setosa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the first stage classifier - setosa:not-setosa\n",
      "Epoch 1 there were 11 errors\n",
      "\n",
      "Perceptron solved the learning problem in 2 epochs\n",
      "The first stage made 33:67 setosa:not_setosa predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the first stage classifier - setosa:not-setosa\")\n",
    "setosa_classifier =  n_input_perceptron(learning_rate=0.05, debug=True)\n",
    "setosa_classifier.fit(train_X, labels=setosa_train_y, max_epochs=20)\n",
    "\n",
    "setosa_predictions = setosa_classifier.predict(train_X)\n",
    "\n",
    "# Count how many we predicted as setosa like this\n",
    "num_setosa_predictions = np.array(setosa_predictions).sum()\n",
    "num_not_setosa_predictions = train_X.shape[0] - num_setosa_predictions\n",
    "\n",
    "print(f\"The first stage made {num_setosa_predictions}:{num_not_setosa_predictions} setosa:not_setosa predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Collect subsets of training data not predicted to be setosa\n",
    "\n",
    "In other words, split the original data and only keep examples/labels for items *not* classified as setosa.   \n",
    "**Run the cell below** to create the subset of data used to train  the **second stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data and labels (not setosa) being passed to second classifier are (67, 4) and 67\n"
     ]
    }
   ],
   "source": [
    "# We found the number of items being passed through in the last cell\n",
    "# use this to allocate new arrays\n",
    "not_setosa_x = np.empty((num_not_setosa_predictions, 4))\n",
    "not_setosa_y = np.empty(num_not_setosa_predictions)\n",
    "\n",
    "# Loop through making copies of every training item not predicted to be setosa\n",
    "# but this time taking the versicolor labels as our targets\n",
    "new_index = 0\n",
    "for i in range(train_X.shape[0]):\n",
    "    if (setosa_predictions[i] == 0):\n",
    "        not_setosa_x[new_index] = train_X[i]\n",
    "        not_setosa_y[new_index] = versicolor_train_y[i]\n",
    "        new_index += 1\n",
    "\n",
    "# Check we got them all\n",
    "assert new_index == num_not_setosa_predictions\n",
    "\n",
    "print (f\"Shape of data and labels (not setosa) being passed to second classifier are {not_setosa_x.shape} and {len(not_setosa_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training a perceptron for the versicolor : virginica task\n",
    "\n",
    "**You may want to run the cell a few times** to get a good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the second stage classifier - versicolor:viginica\n",
      "Epoch 1 there were 35 errors\n",
      "\n",
      "Epoch 2 there were 35 errors\n",
      "\n",
      "Epoch 3 there were 30 errors\n",
      "\n",
      "Epoch 4 there were 25 errors\n",
      "\n",
      "Epoch 5 there were 22 errors\n",
      "\n",
      "Epoch 6 there were 22 errors\n",
      "\n",
      "Epoch 7 there were 19 errors\n",
      "\n",
      "Epoch 8 there were 19 errors\n",
      "\n",
      "Epoch 9 there were 13 errors\n",
      "\n",
      "Epoch 10 there were 15 errors\n",
      "\n",
      "Epoch 11 there were 13 errors\n",
      "\n",
      "Epoch 12 there were 13 errors\n",
      "\n",
      "Epoch 13 there were 15 errors\n",
      "\n",
      "Epoch 14 there were 13 errors\n",
      "\n",
      "Epoch 15 there were 6 errors\n",
      "\n",
      "Epoch 16 there were 15 errors\n",
      "\n",
      "Epoch 17 there were 6 errors\n",
      "\n",
      "Epoch 18 there were 15 errors\n",
      "\n",
      "Epoch 19 there were 6 errors\n",
      "\n",
      "Epoch 20 there were 11 errors\n",
      "\n",
      "Epoch 21 there were 6 errors\n",
      "\n",
      "Epoch 22 there were 11 errors\n",
      "\n",
      "Epoch 23 there were 6 errors\n",
      "\n",
      "Epoch 24 there were 15 errors\n",
      "\n",
      "Epoch 25 there were 6 errors\n",
      "\n",
      "Epoch 26 there were 13 errors\n",
      "\n",
      "Epoch 27 there were 6 errors\n",
      "\n",
      "Epoch 28 there were 13 errors\n",
      "\n",
      "Epoch 29 there were 6 errors\n",
      "\n",
      "Epoch 30 there were 13 errors\n",
      "\n",
      "Epoch 31 there were 4 errors\n",
      "\n",
      "Epoch 32 there were 13 errors\n",
      "\n",
      "Epoch 33 there were 4 errors\n",
      "\n",
      "Epoch 34 there were 8 errors\n",
      "\n",
      "Epoch 35 there were 11 errors\n",
      "\n",
      "Epoch 36 there were 6 errors\n",
      "\n",
      "Epoch 37 there were 8 errors\n",
      "\n",
      "Epoch 38 there were 13 errors\n",
      "\n",
      "Epoch 39 there were 4 errors\n",
      "\n",
      "Epoch 40 there were 8 errors\n",
      "\n",
      "Epoch 41 there were 13 errors\n",
      "\n",
      "Epoch 42 there were 4 errors\n",
      "\n",
      "Epoch 43 there were 8 errors\n",
      "\n",
      "Epoch 44 there were 13 errors\n",
      "\n",
      "Epoch 45 there were 4 errors\n",
      "\n",
      "Epoch 46 there were 6 errors\n",
      "\n",
      "Epoch 47 there were 8 errors\n",
      "\n",
      "Epoch 48 there were 11 errors\n",
      "\n",
      "Epoch 49 there were 4 errors\n",
      "\n",
      "Epoch 50 there were 4 errors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the second stage classifier - versicolor:viginica\")\n",
    "versicolor_classifier =  n_input_perceptron(learning_rate=0.01, debug=True)\n",
    "versicolor_classifier.fit(not_setosa_x, labels=not_setosa_y, max_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Put together our two-stage classifier\n",
    "**Run the cell below to define the new class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Put together our two-stage classifier\n",
    "def CascadePredict(new_item, setosa_classifier, versicolor_classifier):\n",
    "\n",
    "    prediction = -1\n",
    "\n",
    "    # First stage prediction\n",
    "    first_stage_prediction = setosa_classifier.predict_one(new_item)\n",
    "\n",
    "    if first_stage_prediction == 1:  # setosa\n",
    "        prediction = 0\n",
    "    else:\n",
    "        # Second stage prediction\n",
    "        second_stage_prediction = versicolor_classifier.predict_one(new_item)\n",
    "\n",
    "        if second_stage_prediction == 1:\n",
    "            prediction = 1  # versicolor\n",
    "        else:\n",
    "            prediction = 2  # virginica\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluating our 2-stage classifier and visualising the confusion matrix\n",
    "\n",
    "**Run the cell below** it should show the results for the new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy = 94.00 %\n",
      "Final outcome: 47 out of 50 correct test predictions from Cascaded classifier\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGwCAYAAABl+VVyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKYElEQVR4nO3deVxU9f4/8NeAMIPAoKDIIiKoICoKIpbhgrliol5/bomlpW0uZJSi1wR3wlvua9ybWF/MTNPccsncvVqgmCJhKgouXCjJEVS2+fz+8DrXkUEZZ3Bmjq/n43EeD8/2Oe+ZwwPevj+f8zkyIYQAERERkZmzMnUARERERNXBpIWIiIgsApMWIiIisghMWoiIiMgiMGkhIiIii8CkhYiIiCwCkxYiIiKyCLVMHQBVj1qtxvXr1+Ho6AiZTGbqcIiISE9CCNy+fRseHh6wsqqZmsG9e/dQWlpqlLZsbW2hUCiM0paxMGmxENevX4eXl5epwyAiIgPl5uaiYcOGRm/33r178PF2QF5+hVHac3NzQ3Z2tlklLkxaLISjoyMA4MrJxlA6sFdP6v7mF2jqEIjIyMpRhiPYqfl9bmylpaXIy6/AlbTGUDoa9ndCdVsN75DLKC0tZdJC+nvQJaR0sDL4h5HMXy2ZjalDICJj++9Lc2q6i9/BUQYHR8OuoYZ5DkNg0kJERCQhFUKNCgPfKlgh1MYJxsiYtBAREUmIGgJqGJa1GHp+TWE/AxEREVkEVlqIiIgkRA01DO3cMbyFmsGkhYiISEIqhECFMKx7x9Dzawq7h4iIiMgisNJCREQkIVIeiMukhYiISELUEKiQaNLC7iEiIiKyCKy0EBERSQi7h4iIiMgi8OkhIiIiIhNjpYWIiEhC1P9dDG3DHDFpISIikpAKIzw9ZOj5NYVJCxERkYRUCBjhLc/GicXYOKaFiIiIDHLo0CFERkbCw8MDMpkMW7ZsqXRMZmYm+vXrBycnJzg6OuLFF19ETk6OXtdh0kJERCQhaiMt+iguLkabNm2wbNkynfsvXryIjh07onnz5jhw4ABOnz6N6dOnQ6FQ6HUddg8RERFJiBoyVEBmcBv6iIiIQERERJX7p02bhj59+mD+/Pmabb6+vnrHxUoLERER6aRSqbSWkpISvdtQq9XYsWMH/Pz80KtXL7i6uuKFF17Q2YX0JExaiIiIJEQtjLMAgJeXF5ycnDRLQkKC3vHk5+ejqKgIn3zyCXr37o09e/bgb3/7GwYOHIiDBw/q1Ra7h4iIiCSkwgjdQw/Oz83NhVKp1GyXy+V6t6VW3x8h079/f3zwwQcAgKCgIBw7dgyrVq1Cly5dqt0WkxYiIiLSSalUaiUtT6NevXqoVasWWrRoobU9ICAAR44c0astJi1EREQSYsxKizHY2toiNDQUWVlZWtvPnz8Pb29vvdpi0kJERCQhaiGDWhj49JCe5xcVFeHChQua9ezsbKSnp8PZ2RmNGjXCpEmTMHToUHTu3Bldu3bFrl27sG3bNhw4cECv6zBpISIiIoOkpqaia9eumvWYmBgAwMiRI5GcnIy//e1vWLVqFRISEhAdHQ1/f39s2rQJHTt21Os6TFqIiIgkxBTdQ+Hh4RDi8XP/v/nmm3jzzTcNCYtJCxERkZRUwAoVBs5oUmGkWIyNSQsREZGECCOMaREGnl9TOLkcERERWQRWWoiIiCTE3B55NiYmLURERBJSIaxQIQwc0/L4MbUmw+4hIiIisgistBAREUmIGjKoDaxJqGGepRYmLURERBIi5TEt7B4iIiIii8BKCxERkYQYZyAuu4eIiIioht0f02LgCxPZPURERET09FhpISIikhC1Ed49xKeHiIiIqMZxTAsRERFZBDWsJDtPC8e0EBERkUVgpYWIiEhCKoQMFcLAyeUMPL+mMGkhIiKSkAojDMStYPcQERER0dNjpYWIiEhC1MIKagOfHlLz6SEiIiKqaeweIiIiIjIxVlqIiIgkRA3Dn/5RGycUo2PSQkREJCHGmVzOPDtizDMqIiIiokew0kJERCQhxnn3kHnWNJi0EBERSYgaMqhh6JgWzohLRERENYyVFqIadOa4Pb5d4Yrfz9TGzf/YIP5f2Xgp4pZmfy+PIJ3njfn4GgaPLXhGUVJN6jvyDwx+rwDOrmW4cl6BVXEeOPuzg6nDohrAe02GMM9UyoQuX74MmUyG9PR0U4fy3Lh3xwq+Le9i3NyrOvd/nX5Wa4lZkAOZTKDjK7d0Hk+WpUu/Qrw78zq+XuKKsT39cPaEPeakZKO+Z6mpQyMj471+Nh5MLmfoYo7MMyp6roS+fBujYvPQsY/uJMTZtVxr+fduJ7QJK4K7N3/RScHAt//A7q+dsWudC3IvKLAq3hMF123Q9/U/TR0aGRnv9bOhFjKjLOZIsknLxo0bERgYCDs7O7i4uKB79+4oLi4GAKxZswYBAQFQKBRo3rw5VqxYoTnPx8cHABAcHAyZTIbw8HAAgFqtxqxZs9CwYUPI5XIEBQVh165dmvNKS0sxfvx4uLu7Q6FQoHHjxkhISNDsX7BgAQIDA2Fvbw8vLy+MHTsWRUVFz+CbkJbCglr4eZ8SvYbxl5wU1LJRo1nrO0g76Ki1Pe2gI1q0KzZRVFQTeK/JGCQ5puXGjRt49dVXMX/+fPztb3/D7du3cfjwYQghkJSUhPj4eCxbtgzBwcE4deoU3nrrLdjb22PkyJH4+eef0b59e/z4449o2bIlbG1tAQCLFy/GZ599htWrVyM4OBhffPEF+vXrh4yMDDRr1gxLlizB1q1bsWHDBjRq1Ai5ubnIzc3VxGRlZYUlS5agcePGyM7OxtixYzF58mSthOlhJSUlKCkp0ayrVKqa/dIsxN4NzrBzqKiyKkOWRelcAetawF9/aP8q+qugFuq6lpsoKqoJvNfPjtoI3TucXO4ZunHjBsrLyzFw4EA0btwYgYGBGDt2LBwcHDB79mx89tlnGDhwIHx8fDBw4EB88MEHWL16NQCgfv36AAAXFxe4ubnB2dkZAPDpp58iNjYWw4YNg7+/PxITExEUFIRFixYBAHJyctCsWTN07NgR3t7e6NixI1599VVNTBMnTkTXrl3h4+ODl19+GbNnz8aGDRuq/AwJCQlwcnLSLF5eXjX0bVmW3eud8fLfCmGrMM+XedHTefSFsjIZYKbvayMD8V7XvAdveTZ00cehQ4cQGRkJDw8PyGQybNmypcpj33nnHchkMs3fT31IMmlp06YNunXrhsDAQAwePBhJSUkoLCxEQUEBcnNzMXr0aDg4OGiWOXPm4OLFi1W2p1KpcP36dYSFhWltDwsLQ2ZmJgBg1KhRSE9Ph7+/P6Kjo7Fnzx6tY/fv348ePXrA09MTjo6OeP311/Hnn39quqweNXXqVNy6dUuzPFy1eV6dOWGPqxcV6D2cXUNSobppjYpyoG597f9pO9UrR2GBJAvBzy3ea2krLi5GmzZtsGzZsscet2XLFpw4cQIeHh5PdR1JJi3W1tbYu3cvfvjhB7Ro0QJLly6Fv78/Ll26BABISkpCenq6Zjl79iyOHz/+xHZlMu2BSUIIzba2bdsiOzsbs2fPxt27dzFkyBAMGjQIAHDlyhX06dMHrVq1wqZNm5CWlobly5cDAMrKynReSy6XQ6lUai3Pu91fu6BZ6zto0vKeqUMhIykvs8Lvv9ZG2863tba37Xwb51LtTRQV1QTe62enAjKjLMD9/7Q/vDw8bOFhERERmDNnDgYOHFhlXNeuXcP48eORkpICGxubp/pskk1vZTIZwsLCEBYWhri4OHh7e+Po0aPw9PTEpUuXEBUVpfO8B2NYKioqNNuUSiU8PDxw5MgRdO7cWbP92LFjaN++vdZxQ4cOxdChQzFo0CD07t0bN2/eRGpqKsrLy/HZZ5/Byup+nvi4rqHnzd1iK1zPlmvW83JtcfGsHRzrlMO14f2krvi2FQ5tc8Lb8ddNFSbVkO8+r4dJS3Jx/lc7ZKbao8+IP+HqWYYdX7qYOjQyMt7rZ+Npund0tQGg0tCE+Ph4zJgxQ//21Gq89tprmDRpElq2bPnUcUkyaTlx4gT27duHnj17wtXVFSdOnEBBQQECAgIwY8YMREdHQ6lUIiIiAiUlJUhNTUVhYSFiYmLg6uoKOzs77Nq1Cw0bNoRCoYCTkxMmTZqE+Ph4NGnSBEFBQVizZg3S09ORkpICAFi4cCHc3d0RFBQEKysrfPvtt3Bzc0OdOnXQpEkTlJeXY+nSpYiMjMTRo0exatUqE39L5uP86dqYPKipZn31DE8AQI8hN/HRohwAwMHv6wJChq4DCk0SI9Wcg1vrwrFuBaI++A+cXctxJUuBj0f4IP+aralDIyPjvbY8ubm5WpV+uVz+mKOrlpiYiFq1aiE6OtqgeCSZtCiVShw6dAiLFi2CSqWCt7c3PvvsM0RERAAAateujX/84x+YPHky7O3tERgYiIkTJwIAatWqhSVLlmDWrFmIi4tDp06dcODAAURHR0OlUuHDDz9Efn4+WrRoga1bt6JZs2YAAAcHByQmJuL333+HtbU1QkNDsXPnTlhZWSEoKAgLFixAYmIipk6dis6dOyMhIQGvv/66qb4is9LmpSLsvp7+2GP6jPgTfUZwLItUbV9bD9vX1jN1GPQM8F7XvApA071jSBsAjDI8IS0tDYsXL8bJkycrDbPQl0yIR8dykzlSqVRwcnJC4XlfKB0lORSJHlLVqwuIyHKVizIcwPe4detWjYxTfPB34uPjPaFweLoxIw/cKyrDnBf3PFWsMpkMmzdvxoABAwAAixYtQkxMjGZ4BHB/CIaVlRW8vLxw+fLlarctyUoLERHR88rcXpj42muvoXv37lrbevXqhddeew1vvPGGXm0xaSEiIiKDFBUV4cKFC5r17OxspKenw9nZGY0aNYKLi/ZgaxsbG7i5ucHf31+v6zBpISIikhABGdQGjmkRep6fmpqKrl27atZjYmIAACNHjkRycrJBsTyMSQsREZGEmKJ7KDw8HPoMkdVnHMvDOKKTiIiILAIrLURERBKiFjKohWHdQ4aeX1OYtBAREUlIhRHe8mzo+TXFPKMiIiIiegQrLURERBLC7iEiIiKyCGpYQW1gR4qh59cU84yKiIiI6BGstBAREUlIhZChwsDuHUPPrylMWoiIiCSEY1qIiIjIIghhBbWBM+IKI74w0ZjMMyoiIiKiR7DSQkREJCEVkKHCwBcmGnp+TWHSQkREJCFqYfiYFHX13334TLF7iIiIiCwCKy1EREQSojbCQFxDz68pTFqIiIgkRA0Z1AaOSTH0/JpinqkUERER0SNYaSEiIpIQzohLREREFkHKY1rMMyoiIiKiR7DSQkREJCFqGOHdQ2Y6EJdJCxERkYQIIzw9JJi0EBERUU2T8lueOaaFiIiILAIrLURERBIi5aeHmLQQERFJCLuHiIiIiEyMlRYiIiIJkfK7h5i0EBERSQi7h4iIiIhMjJUWIiIiCZFypYVJCxERkYRIOWlh9xAREREZ5NChQ4iMjISHhwdkMhm2bNmi2VdWVobY2FgEBgbC3t4eHh4eeP3113H9+nW9r8OkhYiISEIeVFoMXfRRXFyMNm3aYNmyZZX23blzBydPnsT06dNx8uRJfPfddzh//jz69eun92dj9xAREZGECBj+yLLQ8/iIiAhERETo3Ofk5IS9e/dqbVu6dCnat2+PnJwcNGrUqNrXYdJCREQkIcYc06JSqbS2y+VyyOVyg9oGgFu3bkEmk6FOnTp6ncfuISIiItLJy8sLTk5OmiUhIcHgNu/du4cpU6Zg+PDhUCqVep3LSgsREZGEGLPSkpubq5VYGFplKSsrw7Bhw6BWq7FixQq9z2fSQkREJCHGTFqUSqXe1ZCqlJWVYciQIcjOzsZPP/30VO0yaSEiIqIa9SBh+f3337F//364uLg8VTtMWoiIiCTEFJPLFRUV4cKFC5r17OxspKenw9nZGR4eHhg0aBBOnjyJ7du3o6KiAnl5eQAAZ2dn2NraVvs6TFqIiIgkRAgZhIFJi77np6amomvXrpr1mJgYAMDIkSMxY8YMbN26FQAQFBSkdd7+/fsRHh5e7eswaSEiIiKDhIeHQ4iqZ3d53D59MGkhIiKSEDVkBk8uZ+j5NYVJCxERkYTwhYlEREREJsZKCxERkYSYYiDus8KkhYiISEKk3D3EpIWIiEhCpFxp4ZgWIiIisgistFiYQS91RS2r6s8eSJap+9lLpg6BnqH9g9uaOgR6BkRFCZD1DK5jhO4hc620MGkhIiKSEAHA0LncjDMVnPGxe4iIiIgsAistREREEqKGDDLOiEtERETmjk8PEREREZkYKy1EREQSohYyyDi5HBEREZk7IYzw9JCZPj7E7iEiIiKyCKy0EBERSYiUB+IyaSEiIpIQJi1ERERkEaQ8EJdjWoiIiMgisNJCREQkIVJ+eohJCxERkYTcT1oMHdNipGCMjN1DREREZBFYaSEiIpIQPj1EREREFkH8dzG0DXPE7iEiIiKyCKy0EBERSQi7h4iIiMgySLh/iEkLERGRlBih0gIzrbRwTAsRERFZBFZaiIiIJIQz4hIREZFFkPJAXHYPERERkUVg0kJERCQlQmacRQ+HDh1CZGQkPDw8IJPJsGXLFu2QhMCMGTPg4eEBOzs7hIeHIyMjQ++PxqSFiIhIQh6MaTF00UdxcTHatGmDZcuW6dw/f/58LFiwAMuWLcMvv/wCNzc39OjRA7dv39brOhzTQkRERDqpVCqtdblcDrlcXum4iIgIRERE6GxDCIFFixZh2rRpGDhwIABg7dq1aNCgAdatW4d33nmn2vGw0kJERCQlwkgLAC8vLzg5OWmWhIQEvcPJzs5GXl4eevbsqdkml8vRpUsXHDt2TK+2WGkhIiKSEGM+PZSbmwulUqnZrqvK8iR5eXkAgAYNGmhtb9CgAa5cuaJXW9VKWpYsWVLtBqOjo/UKgIiIiMyTUqnUSloMIZNpJ1JCiErbnqRaScvChQurHRCTFiIiIhMzo8nh3NzcANyvuLi7u2u25+fnV6q+PEm1kpbs7Gy9GiUiIiLTMLfJ5Xx8fODm5oa9e/ciODgYAFBaWoqDBw8iMTFRr7aeekxLaWkpsrOz0aRJE9SqxaExREREZsEEb3kuKirChQsXNOvZ2dlIT0+Hs7MzGjVqhIkTJ2LevHlo1qwZmjVrhnnz5qF27doYPny4XtfRO9u4c+cOJkyYgLVr1wIAzp8/D19fX0RHR8PDwwNTpkzRt0kiIiKyYKmpqejatatmPSYmBgAwcuRIJCcnY/Lkybh79y7Gjh2LwsJCvPDCC9izZw8cHR31uo7ejzxPnToVp0+fxoEDB6BQKDTbu3fvjm+++Ubf5oiIiMioZEZaqi88PBxCiEpLcnLy/YhkMsyYMQM3btzAvXv3cPDgQbRq1UrvT6Z3pWXLli345ptv8OKLL2qN+m3RogUuXryodwBERERkRCboHnpW9K60FBQUwNXVtdL24uJivR9dIiIiIqouvZOW0NBQ7NixQ7P+IFFJSkpChw4djBcZERER6c+IM+KaG727hxISEtC7d2+cO3cO5eXlWLx4MTIyMvDvf/8bBw8erIkYiYiIqLqe4i3NOtswQ3pXWl566SUcPXoUd+7cQZMmTbBnzx40aNAA//73vxESElITMRIRERE93TwtgYGBmkeeiYiIyHwIcX8xtA1z9FRJS0VFBTZv3ozMzEzIZDIEBASgf//+nGSOiIjI1CT89JDeWcbZs2fRv39/5OXlwd/fH8D9Cebq16+PrVu3IjAw0OhBEhEREek9pmXMmDFo2bIlrl69ipMnT+LkyZPIzc1F69at8fbbb9dEjERERFRdDwbiGrqYIb0rLadPn0Zqairq1q2r2Va3bl3MnTsXoaGhRg2OiIiI9CMT9xdD2zBHelda/P398Z///KfS9vz8fDRt2tQoQREREdFTkvA8LdVKWlQqlWaZN28eoqOjsXHjRly9ehVXr17Fxo0bMXHiRL1fMU1ERERUXdXqHqpTp47WFP1CCAwZMkSzTfz32ajIyEhUVFTUQJhERERULRKeXK5aScv+/ftrOg4iIiIyhuf9kecuXbrUdBxEREREj/XUs8HduXMHOTk5KC0t1dreunVrg4MiIiKip/S8V1oeVlBQgDfeeAM//PCDzv0c00JERGRCEk5a9H7keeLEiSgsLMTx48dhZ2eHXbt2Ye3atWjWrBm2bt1aEzESERER6V9p+emnn/D9998jNDQUVlZW8Pb2Ro8ePaBUKpGQkIBXXnmlJuIkIiKi6pDw00N6V1qKi4vh6uoKAHB2dkZBQQGA+29+PnnypHGjIyIiIr08mBHX0MUcPdWMuFlZWQCAoKAgrF69GteuXcOqVavg7u5u9ACN6fLly5DJZEhPTzfL9uh/WrUtRPySdHy19xB2nv4RHbrmmzokMpLCVGukj7PDoa72+LGVI/L3VV3wzZwpx4+tHJHzlc0zjJBqypBXf8Oi5fuwcdsWrNu4DdNnHYNnw9umDossyFONablx4wYAID4+Hrt27UKjRo2wZMkSzJs3z+gBGpOXlxdu3LiBVq1amToUegKFXQWysxyw8pPmpg6FjKziLuDgX4Hmfy957HH5+2rh1q/WkLuqn1FkVNNatS7A9q1NEDO+K6ZN7gRrazXmzj8MuaLc1KFJi4Sn8dd7TEtUVJTm38HBwbh8+TJ+++03NGrUCPXq1TNqcPoqKyuDjU3V/yOztraGm5vbM4zoyUpLS2Fra2vqMMxO6tF6SD1q2p8nqhn1OlWgXqfHP2V47z8yZM2TI3j1XaSPtXtGkVFNi5vaSWt9wfxQrP9uG5o1K8TZM/VNFBVZEr0rLY+qXbs22rZtq3fCsnr1anh6ekKt1v5fVL9+/TBy5EgAwLZt2xASEgKFQgFfX1/MnDkT5eX/y8hlMhlWrVqF/v37w97eHnPmzEFhYSGioqJQv3592NnZoVmzZlizZg0A3d05GRkZeOWVV6BUKuHo6IhOnTrh4sWLAAC1Wo1Zs2ahYcOGkMvlCAoKwq5dux77uQ4ePIj27dtDLpfD3d0dU6ZM0Yo5PDwc48ePR0xMDOrVq4cePXro9b0RSZ1QAxlTFfAeVQqHpqyySJm9fRkA4PZt/sfNmGQwwpgWU3+IKlSr0hITE1PtBhcsWFCt4wYPHozo6Gjs378f3bp1AwAUFhZi9+7d2LZtG3bv3o0RI0ZgyZIlmkTi7bffBnC/W+qB+Ph4JCQkYOHChbC2tsb06dNx7tw5/PDDD6hXrx4uXLiAu3fv6ozh2rVr6Ny5M8LDw/HTTz9BqVTi6NGjmiRj8eLF+Oyzz7B69WoEBwfjiy++QL9+/ZCRkYFmzZrpbK9Pnz4YNWoUvvzyS/z222946623oFAoMGPGDM1xa9euxXvvvYejR49q3tv0qJKSEpSU/K98rlKpqvW9Elm6y/+yhcwa8BpRZupQqEYJvPXeaZw944Irl51MHQxZiGolLadOnapWYw+/VPFJnJ2d0bt3b6xbt06TtHz77bdwdnZGt27d0LVrV0yZMkVTdfH19cXs2bMxefJkraRl+PDhePPNNzXrOTk5CA4ORrt27QAAjRs3rjKG5cuXw8nJCevXr9d0K/n5+Wn2f/rpp4iNjcWwYcMAAImJidi/fz8WLVqE5cuXV2pvxYoV8PLywrJlyyCTydC8eXNcv34dsbGxiIuLg5XV/cJW06ZNMX/+/Md+PwkJCZg5c+ZjjyGSGlWGFXL/zwYvfHsHevw6IQs0NjodPr638NH74aYORXok/MizSV+YGBUVhbfffhsrVqyAXC5HSkoKhg0bBmtra6SlpeGXX37B3LlzNcdXVFTg3r17uHPnDmrXrg0AmuTkgffeew//7//9P5w8eRI9e/bEgAED8NJLL+m8fnp6Ojp16qRzHIxKpcL169cRFhamtT0sLAynT5/W2V5mZiY6dOiglbyFhYWhqKgIV69eRaNGjXTGrMvUqVO1KlwqlQpeXl5PPI/Ikv110hqlN2U40sNes01UyHD+H3LkfGWLjnuKTRgdGcu740/hhQ7XMfmDcPz5R21ThyM9Ep4R96nfPWQMkZGRUKvV2LFjB0JDQ3H48GFN95JarcbMmTMxcODASucpFArNv+3t7bX2RURE4MqVK9ixYwd+/PFHdOvWDePGjcOnn35aqR07uycP8Hu0eiSEqLKipGvfg+6fh7c/GrMucrkccrn8iccRSYlbZBmcX9QepHvqHTu4RZbBYwC7iyyfwHsT0tGh4zVMiemC/+Q9+Xch0cNMmrTY2dlh4MCBSElJwYULF+Dn54eQkBAAQNu2bZGVlYWmTZvq3W79+vUxatQojBo1Cp06dcKkSZN0Ji2tW7fG2rVrdT51pFQq4eHhgSNHjqBz586a7ceOHUP79u11XrdFixbYtGmTVvJy7NgxODo6wtPTU+/P8TxT2JXDo9H/xiI18LwLX//buH3LBgV5isecSeau/A5wN+d/zwDcvSbD7d+sYOMkoHAXsK2jPfhWVguQ1xOw9zHT//pRtY2NPoXwbrmYNf0l3L1jg7p17wEAiottUFpqbeLoJISVlpoTFRWFyMhIZGRkYMSIEZrtcXFx6Nu3L7y8vDB48GBYWVnh119/xZkzZzBnzpwq24uLi0NISAhatmyJkpISbN++HQEBATqPHT9+PJYuXYphw4Zh6tSpcHJywvHjx9G+fXv4+/tj0qRJiI+PR5MmTRAUFIQ1a9YgPT0dKSkpOtsbO3YsFi1ahAkTJmD8+PHIyspCfHw8YmJiNONZqHqatVQh8V//m2H57Um/AwD2fu+OhXEtTRUWGYHqrDVOvvm/LoHf599PQt37l6Hl3HumCouegb79LwEA5i88qLV9wfx2+HF3YxNEJE3GmNHWXGfENXnS8vLLL8PZ2RlZWVkYPny4ZnuvXr2wfft2zJo1C/Pnz4eNjQ2aN2+OMWPGPLY9W1tbTJ06FZcvX4adnR06deqE9evX6zzWxcUFP/30EyZNmoQuXbrA2toaQUFBmnEs0dHRUKlU+PDDD5Gfn48WLVpg69atOp8cAgBPT0/s3LkTkyZNQps2beDs7IzRo0fj448/fspv5/l1JtUZfdp0N3UYVAOc21eg+9nqz4LKcSzS0afbIFOHQBZOJqp65pbMikqlgpOTE7rVG41aVpzTQOq67r9k6hDoGdo/uK2pQ6BnoLyiBPuyFuDWrVtQKpVGb//B34nGc+bCSmFYN7r63j1c/nhajcX6tJ6qz+Krr75CWFgYPDw8cOXKFQDAokWL8P333xs1OCIiItKTCabxLy8vx8cffwwfHx/Y2dnB19cXs2bNqjSBrKH0TlpWrlyJmJgY9OnTB3/99RcqKu6P9K9Tpw4WLVpk1OCIiIjI/CUmJmLVqlVYtmwZMjMzMX/+fPzjH//A0qVLjXodvZOWpUuXIikpCdOmTYO19f9Ge7dr1w5nzpwxanBERESkH4On8H+Kgbz//ve/0b9/f7zyyito3LgxBg0ahJ49eyI1NdWon03vpCU7OxvBwcGVtsvlchQXc8AcERGRST2YEdfQBffHyTy8PPx6mYd17NgR+/btw/nz5wEAp0+fxpEjR9CnTx+jfjS9kxYfHx+tFw4+8MMPP6BFixbGiImIiIielhHHtHh5ecHJyUmzJCQk6LxkbGwsXn31VTRv3hw2NjYIDg7GxIkT8eqrrxr1o+n9yPOkSZMwbtw43Lt3D0II/Pzzz/j666+RkJCAf/7zn0YNjoiIiEwnNzdX6+mhqmZq/+abb/B///d/WLduHVq2bIn09HRMnDgRHh4emncIGoPeScsbb7yB8vJyTJ48GXfu3MHw4cPh6emJxYsXa14sSERERKZhzMnllEpltR55njRpEqZMmaLJAwIDA3HlyhUkJCSYNmkBgLfeegtvvfUW/vjjD6jVari6uhotICIiIjKACabxv3PnTqWZ362trY3+yLNBM+LWq1fPWHEQERGRhYqMjMTcuXPRqFEjtGzZEqdOncKCBQvw5ptvGvU6eictPj4+Vb7lGAAuXeJMnkRERCZjhO4hfSstS5cuxfTp0zF27Fjk5+fDw8MD77zzDuLi4gwMRJveScvEiRO11svKynDq1Cns2rULkyZNMlZcRERE9DRM0D3k6OiIRYsW1fgks3onLe+//77O7cuXLzf6JDJEREREDzzVu4d0iYiIwKZNm4zVHBERET0NE7x76FkxaCDuwzZu3AhnZ2djNUdERERPwZiPPJsbvZOW4OBgrYG4Qgjk5eWhoKAAK1asMGpwRERERA/onbQMGDBAa93Kygr169dHeHg4mjdvbqy4iIiIiLTolbSUl5ejcePG6NWrF9zc3GoqJiIiInpaJnh66FnRayBurVq18N5771X5lkciIiIyrQdjWgxdzJHeTw+98MILOHXqVE3EQkRERFQlvce0jB07Fh9++CGuXr2KkJAQ2Nvba+1v3bq10YIjIiKip2CmlRJDVTtpefPNN7Fo0SIMHToUABAdHa3ZJ5PJIISATCZDRUWF8aMkIiKi6pHwmJZqJy1r167FJ598guzs7JqMh4iIiEinaictQtxPu7y9vWssGCIiIjIMJ5f7r8e93ZmIiIjMALuH7vPz83ti4nLz5k2DAiIiIiLSRa+kZebMmXBycqqpWIiIiMhA7B76r2HDhsHV1bWmYiEiIiJDSbh7qNqTy3E8CxEREZmS3k8PERERkRmTcKWl2kmLWq2uyTiIiIjICDimhYiIiCyDhCster8wkYiIiMgUWGkhIiKSEglXWpi0EBERSYiUx7Swe4iIiIgsAistREREUsLuISIiIrIE7B4iIiIiMjFWWoiIiKSE3UNERERkESSctLB7iIiIiCwCKy1EREQSIvvvYmgb5ohJCxERkZRIuHuISQsREZGE8JFnIiIiose4du0aRowYARcXF9SuXRtBQUFIS0sz6jVYaSEiIpISE3QPFRYWIiwsDF27dsUPP/wAV1dXXLx4EXXq1DEwEG1MWoiIiKTGSN07KpVKa10ul0Mul1c6LjExEV5eXlizZo1mW+PGjY0TxEPYPUREREQ6eXl5wcnJSbMkJCToPG7r1q1o164dBg8eDFdXVwQHByMpKcno8bDSQkREJCHGHIibm5sLpVKp2a6rygIAly5dwsqVKxETE4O///3v+PnnnxEdHQ25XI7XX3/dsGAewqSFiIhISow4pkWpVGolLVVRq9Vo164d5s2bBwAIDg5GRkYGVq5cadSkhd1DREREZBB3d3e0aNFCa1tAQABycnKMeh1WWoiIiCTEFPO0hIWFISsrS2vb+fPn4e3tbVggj2ClhYiISEqEkRY9fPDBBzh+/DjmzZuHCxcuYN26dfj8888xbtw4o3ykB5i0EBERkUFCQ0OxefNmfP3112jVqhVmz56NRYsWISoqyqjXYfeQhan44w/IZDamDoNq2I+tHE0dAj1DS68kmzoEegaKbqsR2rLmr2Oqafz79u2Lvn37GnbhJ2DSQkREJCV8YSIRERFZBAknLRzTQkRERBaBlRYiIiIJMdWYlmeBSQsREZGUsHuIiIiIyLRYaSEiIpIQmRCQCcNKJYaeX1OYtBAREUkJu4eIiIiITIuVFiIiIgnh00NERERkGdg9RERERGRarLQQERFJCLuHiIiIyDJIuHuISQsREZGESLnSwjEtREREZBFYaSEiIpISdg8RERGRpTDX7h1DsXuIiIiILAIrLURERFIixP3F0DbMEJMWIiIiCeHTQ0REREQmxkoLERGRlPDpISIiIrIEMvX9xdA2zBG7h4iIiMgisNJCREQkJeweIiIiIksg5aeHmLQQERFJiYTnaeGYFiIiIrIIrLQQERFJCLuHiIiIyDJIeCAuu4eIiIjIIjBpISIikpAH3UOGLk8rISEBMpkMEydONNpneoDdQ0RERFJiwqeHfvnlF3z++edo3bq1YdevAistREREZLCioiJERUUhKSkJdevWrZFrMGkhIiKSEGN2D6lUKq2lpKSkyuuOGzcOr7zyCrp3715jn41JCxERkZQIIy0AvLy84OTkpFkSEhJ0XnL9+vU4efJklfuNhWNaiIiISKfc3FwolUrNulwu13nM+++/jz179kChUNRoPExaiIiIJMSYk8splUqtpEWXtLQ05OfnIyQkRLOtoqIChw4dwrJly1BSUgJra2vDAvovJi1ERERSohb3F0PbqKZu3brhzJkzWtveeOMNNG/eHLGxsUZLWAAmLURERNLyjGfEdXR0RKtWrbS22dvbw8XFpdJ2Q3EgLhEREVkEVlqIiIgkRAYjjGkxMIYDBw4Y2IJuTFqIiIikxIQz4tY0dg8RERGRRWClhYiISEKM+cizuWHSQkREJCXP+OmhZ4ndQ0RERGQRWGkhIiKSEJkQkBk4kNbQ82sKkxYiIiIpUf93MbQNM8TuISIiIrIIrLQQERFJCLuHiIiIyDJI+OkhJi1ERERSwhlxiYiIiEyLlRYiIiIJ4Yy4RCbQd+QfGPxeAZxdy3DlvAKr4jxw9mcHU4dFNYD3WpounFBi32pP5JxxgCrfFmM+z0SbXje1jsn73Q7ff9IYF04oIdQyuPvdwRvLf4OzZ6mJopYAdg+ZnxkzZiAoKMjgdg4cOACZTIa//vqr2ueMGjUKAwYMMPjaVLUu/Qrx7szr+HqJK8b29MPZE/aYk5KN+vxFJjm819JVcscKngHFGDzros79BVcUWDgoEA2a3EH0+rOYsusUek3IhY3cPP9gkulZbKXlo48+woQJEwxu56WXXsKNGzfg5ORU7XMWL14MYaZZqFQMfPsP7P7aGbvWuQAAVsV7IiT8Nvq+/ifWJLibODoyJt5r6WrZ9S+07PpXlfu3/6MRWnYtxIC/X9Fsq9eo5BlEJm0y9f3F0DbMkcVWWhwcHODi4lLl/tLS6v0vzdbWFm5ubpDJZNW+tpOTE+rUqVPt40k/tWzUaNb6DtIOOmptTzvoiBbtik0UFdUE3uvnl1oNZPzkDFefu1j+WgtMbRuKT/u3xundzqYOzfI96B4ydDFDZpu0rF69Gp6enlCrtdO9fv36YeTIkZW6hx502SQkJMDDwwN+fn4AgGPHjiEoKAgKhQLt2rXDli1bIJPJkJ6eDqBy91BycjLq1KmD3bt3IyAgAA4ODujduzdu3LhR6VoPqNVqJCYmomnTppDL5WjUqBHmzp2r2R8bGws/Pz/Url0bvr6+mD59OsrKyh77+UtKSqBSqbSW54XSuQLWtYC//tAuBP5VUAt1XctNFBXVBN7r51fRHzYoKbbG3pUNEdDlL4z76hxa9/oT/3qnOX4/rjR1eGSmzDZpGTx4MP744w/s379fs62wsBC7d+9GVFSUznP27duHzMxM7N27F9u3b8ft27cRGRmJwMBAnDx5ErNnz0ZsbOwTr33nzh18+umn+Oqrr3Do0CHk5OTgo48+qvL4qVOnIjExEdOnT8e5c+ewbt06NGjQQLPf0dERycnJOHfuHBYvXoykpCQsXLjwsTEkJCTAyclJs3h5eT0xbql5NNGXyWC2Ex6RYXivnz9C3K9uB/a4iZfHXEfDlsXoOfYaWnYrxJEUNxNHZ+GEkRYzZLZjWpydndG7d2+sW7cO3bp1AwB8++23cHZ2Rrdu3XDs2LFK59jb2+Of//wnbG1tAQCrVq2CTCZDUlISFAoFWrRogWvXruGtt9567LXLysqwatUqNGnSBAAwfvx4zJo1S+ext2/fxuLFi7Fs2TKMHDkSANCkSRN07NhRc8zHH3+s+Xfjxo3x4Ycf4ptvvsHkyZOrjGHq1KmIiYnRrKtUqucmcVHdtEZFOVC3vvb/tJ3qlaOwwGx/ZOkp8F4/v+zrlsGqlhpuze5obXdregcXf2GlxRBSnsbfbCstABAVFYVNmzahpOT+wKyUlBQMGzYM1tbWOo8PDAzUJCwAkJWVhdatW0OhUGi2tW/f/onXrV27tiZhAQB3d3fk5+frPDYzMxMlJSWaxEqXjRs3omPHjnBzc4ODgwOmT5+OnJycx8Ygl8uhVCq1ludFeZkVfv+1Ntp2vq21vW3n2ziXam+iqKgm8F4/v2rZCni3LkL+JTut7fnZdnD25GBc0s2sk5bIyEio1Wrs2LEDubm5OHz4MEaMGFHl8fb22r/khBCVBthW56kfGxsbrXWZTFbleXZ2djq3P3D8+HEMGzYMERER2L59O06dOoVp06ZVe6Dw8+q7z+uh9/Cb6DnsT3g1vYd3ZlyDq2cZdnxZ9eBrsky819JVUmyFqxn2uJpx/3fzn7kKXM2wx81r9/9z2e2dazi5vR6Oft0ABZcVOJjshrM/OqPT6zce1yw9iYQH4pp1/dXOzg4DBw5ESkoKLly4AD8/P4SEhFT7/ObNmyMlJQUlJSWQy+UAgNTUVKPG2KxZM9jZ2WHfvn0YM2ZMpf1Hjx6Ft7c3pk2bptl25cqVSseRtoNb68KxbgWiPvgPnF3LcSVLgY9H+CD/mu2TTyaLwnstXTm/OmDJsEDN+ubZPgCA9oP+g9c+u4A2vW9i6NyL2LuiITbF+8C1yV2MXvUbmoTerqpJqg4BwNBHls0zZzHvpAW430UUGRmJjIyMx1ZZdBk+fDimTZuGt99+G1OmTEFOTg4+/fRTANDrEefHUSgUiI2NxeTJk2Fra4uwsDAUFBQgIyMDo0ePRtOmTZGTk4P169cjNDQUO3bswObNm41ybanbvrYetq+tZ+ow6BngvZamZh1UWHrl6GOP6TA0Hx2G6u5+p6fDMS0m9PLLL8PZ2RlZWVkYPny4XucqlUps27YN6enpCAoKwrRp0xAXFwcAWuNcDDV9+nR8+OGHiIuLQ0BAAIYOHaoZA9O/f3988MEHGD9+PIKCgnDs2DFMnz7daNcmIiJ6XsjEcza1a0pKCt544w3cunXrieNRzIlKpYKTkxPC0R+1ZDZPPoGILMaTqhEkDUW31Qht+R/cunWrRh6uePB34uWgKahlLTeorfKKEvyU/kmNxfq0zL57yFBffvklfH194enpidOnTyM2NhZDhgyxqISFiIio2iT8wkTJJy15eXmIi4tDXl4e3N3dMXjwYK3ZaomIiMgySD5pmTx58mMncSMiIpIUNQBDnzUx0xcmSj5pISIiep7w6SEiIiIiE2OlhYiISEo4EJeIiIgsgoSTFnYPERERkUESEhIQGhoKR0dHuLq6YsCAAcjKyjL6dZi0EBERSYkJXph48OBBjBs3DsePH8fevXtRXl6Onj17ori42Kgfjd1DREREUmLER55VKpXWZrlcrnkB8cN27dqltb5mzRq4uroiLS0NnTt3NjCY/2GlhYiISEIePPJs6AIAXl5ecHJy0iwJCQnViuHWrVsAAGdnZ6N+NlZaiIiISKfc3Fytdw/pqrI8SgiBmJgYdOzYEa1atTJqPExaiIiIpMSITw8plUq9X5g4fvx4/Prrrzhy5IhhMejApIWIiEhK1AKQGZi0qJ/u/AkTJmDr1q04dOgQGjZsaFgMOjBpISIiIoMIITBhwgRs3rwZBw4cgI+PT41ch0kLERGRlJhgcrlx48Zh3bp1+P777+Ho6Ii8vDwAgJOTE+zs7AyL5SF8eoiIiEhSjDFHi35Jy8qVK3Hr1i2Eh4fD3d1ds3zzzTdG/WSstBAREZFBxDOa9p9JCxERkZRI+N1DTFqIiIikRK1/947uNswPx7QQERGRRWClhYiISEqE+v5iaBtmiEkLERGRlHBMCxEREVkEjmkhIiIiMi1WWoiIiKSE3UNERERkEQSMkLQYJRKjY/cQERERWQRWWoiIiKSE3UNERERkEdRqAAbOs6I2z3la2D1EREREFoGVFiIiIilh9xARERFZBAknLeweIiIiIovASgsREZGUSHgafyYtREREEiKEGsLAtzQben5NYdJCREQkJUIYXinhmBYiIiKip8dKCxERkZQII4xpMdNKC5MWIiIiKVGrAZmBY1LMdEwLu4eIiIjIIrDSQkREJCXsHiIiIiJLINRqCAO7h8z1kWd2DxEREZFFYKWFiIhIStg9RERERBZBLQCZNJMWdg8RERGRRWClhYiISEqEAGDoPC3mWWlh0kJERCQhQi0gDOweEkxaiIiIqMYJNQyvtPCRZyIiIpKwFStWwMfHBwqFAiEhITh8+LBR22fSQkREJCFCLYyy6Oubb77BxIkTMW3aNJw6dQqdOnVCREQEcnJyjPbZmLQQERFJiVAbZ9HTggULMHr0aIwZMwYBAQFYtGgRvLy8sHLlSqN9NI5psRAPBkWVo8zgOYOIyLwU3TbP8QNkXEVF9+9zTQ9yNcbfiXKUAQBUKpXWdrlcDrlcXun40tJSpKWlYcqUKVrbe/bsiWPHjhkWzEOYtFiI27dvAwCOYKeJIyEiYwttaeoI6Fm6ffs2nJycjN6ura0t3NzccCTPOH8nHBwc4OXlpbUtPj4eM2bMqHTsH3/8gYqKCjRo0EBre4MGDZCXl2eUeAAmLRbDw8MDubm5cHR0hEwmM3U4z4xKpYKXlxdyc3OhVCpNHQ7VIN7r58fzeq+FELh9+zY8PDxqpH2FQoHs7GyUlpYapT0hRKW/N7qqLA979HhdbRiCSYuFsLKyQsOGDU0dhskolcrn6pfb84z3+vnxPN7rmqiwPEyhUEChUNToNXSpV68erK2tK1VV8vPzK1VfDMGBuERERGQQW1tbhISEYO/evVrb9+7di5deeslo12GlhYiIiAwWExOD1157De3atUOHDh3w+eefIycnB++++67RrsGkhcyaXC5HfHz8E/tRyfLxXj8/eK+laejQofjzzz8xa9Ys3LhxA61atcLOnTvh7e1ttGvIhLm+YICIiIjoIRzTQkRERBaBSQsRERFZBCYtREREZBGYtBCRSVy+fBkymQzp6elm2R79z4wZMxAUFGRwOwcOHIBMJsNff/1V7XNGjRqFAQMGGHxtkgYOxCWzcPnyZfj4+ODUqVNG+eVI5q+iogIFBQWoV68eatUy/EFG/gzVnKKiIpSUlMDFxcWgdkpLS3Hz5k00aNCg2rOk3rp1C0II1KlTx6BrkzTwkWciqhFlZWWwsbGpcr+1tTXc3NyeYURPVlpaCltbW1OHYXYcHBzg4OBQ5f7qfm8P3o2jj5qeQZYsC7uHyKg2btyIwMBA2NnZwcXFBd27d0dxcTEAYM2aNQgICIBCoUDz5s2xYsUKzXk+Pj4AgODgYMhkMoSHhwMA1Go1Zs2ahYYNG0IulyMoKAi7du3SnFdaWorx48fD3d0dCoUCjRs3RkJCgmb/ggULEBgYCHt7e3h5eWHs2LEoKip6Bt+EZVm9ejU8PT2hVmu/bbhfv34YOXIkAGDbtm0ICQmBQqGAr68vZs6cifLycs2xMpkMq1atQv/+/WFvb485c+agsLAQUVFRqF+/Puzs7NCsWTOsWbMGgO7unIyMDLzyyitQKpVwdHREp06dcPHiRQBP/lnQ5eDBg2jfvj3kcjnc3d0xZcoUrZjDw8Mxfvx4xMTEoF69eujRo4dB36OletL9f7R76EGXTUJCAjw8PODn5wcAOHbsGIKCgqBQKNCuXTts2bJF6x4/2j2UnJyMOnXqYPfu3QgICICDgwN69+6NGzduVLrWA2q1GomJiWjatCnkcjkaNWqEuXPnavbHxsbCz88PtWvXhq+vL6ZPn46ysjLjfmFkOoLISK5fvy5q1aolFixYILKzs8Wvv/4qli9fLm7fvi0+//xz4e7uLjZt2iQuXbokNm3aJJydnUVycrIQQoiff/5ZABA//vijuHHjhvjzzz+FEEIsWLBAKJVK8fXXX4vffvtNTJ48WdjY2Ijz588LIYT4xz/+Iby8vMShQ4fE5cuXxeHDh8W6des0MS1cuFD89NNP4tKlS2Lfvn3C399fvPfee8/+yzFzf/75p7C1tRU//vijZtvNmzeFra2t2L17t9i1a5dQKpUiOTlZXLx4UezZs0c0btxYzJgxQ3M8AOHq6ir+9a9/iYsXL4rLly+LcePGiaCgIPHLL7+I7OxssXfvXrF161YhhBDZ2dkCgDh16pQQQoirV68KZ2dnMXDgQPHLL7+IrKws8cUXX4jffvtNCPHknwVd7dWuXVuMHTtWZGZmis2bN4t69eqJ+Ph4TcxdunQRDg4OYtKkSeK3334TmZmZNfgtm68n3f/4+HjRpk0bzb6RI0cKBwcH8dprr4mzZ8+KM2fOCJVKJZydncWIESNERkaG2Llzp/Dz89O6J/v37xcARGFhoRBCiDVr1ggbGxvRvXt38csvv4i0tDQREBAghg8frnWt/v37a9YnT54s6tatK5KTk8WFCxfE4cOHRVJSkmb/7NmzxdGjR0V2drbYunWraNCggUhMTKyR742ePSYtZDRpaWkCgLh8+XKlfV5eXlrJhBD3f7l06NBBCFH5D84DHh4eYu7cuVrbQkNDxdixY4UQQkyYMEG8/PLLQq1WVyvGDRs2CBcXl+p+pOdKv379xJtvvqlZX716tXBzcxPl5eWiU6dOYt68eVrHf/XVV8Ld3V2zDkBMnDhR65jIyEjxxhtv6Lzeo/d86tSpwsfHR5SWluo8/kk/C4+29/e//134+/tr/WwsX75cODg4iIqKCiHE/aQlKCioqq/kufK4+68raWnQoIEoKSnRbFu5cqVwcXERd+/e1WxLSkp6YtICQFy4cEFzzvLly0WDBg20rvUgaVGpVEIul2slKU8yf/58ERISUu3jybyxe4iMpk2bNujWrRsCAwMxePBgJCUlobCwEAUFBcjNzcXo0aM1feMODg6YM2eOpvSvi0qlwvXr1xEWFqa1PSwsDJmZmQDul47T09Ph7++P6Oho7NmzR+vY/fv3o0ePHvD09ISjoyNef/11/Pnnn5ouK/qfqKgobNq0CSUlJQCAlJQUDBs2DNbW1khLS8OsWbO07t9bb72FGzdu4M6dO5o22rVrp9Xme++9h/Xr1yMoKAiTJ0/GsWPHqrx+eno6OnXqpHMcTHV+Fh6VmZmJDh06aA34DAsLQ1FREa5evVplzM+rx91/XQIDA7XGsWRlZaF169Zabxhu3779E69bu3ZtNGnSRLPu7u6O/Px8ncdmZmaipKQE3bp1q7K9jRs3omPHjnBzc4ODgwOmT5+OnJycJ8ZBloFJCxmNtbU19u7dix9++AEtWrTA0qVL4e/vj0uXLgEAkpKSkJ6erlnOnj2L48ePP7HdR58yEEJotrVt2xbZ2dmYPXs27t69iyFDhmDQoEEAgCtXrqBPnz5o1aoVNm3ahLS0NCxfvhwA2MetQ2RkJNRqNXbs2IHc3FwcPnwYI0aMAHB/HMHMmTO17t+ZM2fw+++/a/2Rsre312ozIiICV65cwcSJE3H9+nV069YNH330kc7r29nZPTHGx/0sPErXPvHfhyUf3v5ozM+rx91/XR793h73fT/Oo0mqTCar8rwn/YwcP34cw4YNQ0REBLZv345Tp05h2rRpKC0tfWIcZBn49BAZlUwmQ1hYGMLCwhAXFwdvb28cPXoUnp6euHTpEqKionSe9+B/bBUVFZptSqUSHh4eOHLkCDp37qzZfuzYMa3/wSmVSgwdOhRDhw7FoEGD0Lt3b9y8eROpqakoLy/HZ599Biur+/n5hg0bauJjS4KdnR0GDhyIlJQUXLhwAX5+fggJCQFwPznMyspC06ZN9W63fv36GDVqFEaNGoVOnTph0qRJ+PTTTysd17p1a6xdu1bnU0fV/Vl4WIsWLbBp0yatP6bHjh2Do6MjPD099f4cUve4+18dzZs3R0pKCkpKSjQvQkxNTTVqjM2aNYOdnR327duHMWPGVNp/9OhReHt7Y9q0aZptV65cMWoMZFpMWshoTpw4gX379qFnz55wdXXFiRMnUFBQgICAAMyYMQPR0dFQKpWIiIhASUkJUlNTUVhYiJiYGLi6usLOzg67du1Cw4YNoVAo4OTkhEmTJiE+Ph5NmjRBUFAQ1qxZg/T0dKSkpAAAFi5cCHd3dwQFBcHKygrffvst3NzcUKdOHTRp0gTl5eVYunQpIiMjcfToUaxatcrE35J5i4qKQmRkJDIyMrT+lx0XF4e+ffvCy8sLgwcPhpWVFX799VecOXMGc+bMqbK9uLg4hISEoGXLligpKcH27dsREBCg89jx48dj6dKlGDZsGKZOnQonJyccP34c7du3h7+//xN/Fh41duxYLFq0CBMmTMD48eORlZWF+Ph4xMTEaJJY0lbV/a+O4cOHY9q0aXj77bcxZcoU5OTkaJLT6s7J8iQKhQKxsbGYPHkybG1tERYWhoKCAmRkZGD06NFo2rQpcnJysH79eoSGhmLHjh3YvHmzUa5NZsJ0w2lIas6dOyd69eol6tevL+RyufDz8xNLly7V7E9JSRFBQUHC1tZW1K1bV3Tu3Fl89913mv1JSUnCy8tLWFlZiS5dugghhKioqBAzZ84Unp6ewsbGRrRp00b88MMPmnM+//xzERQUJOzt7YVSqRTdunUTJ0+e1OxfsGCBcHd3F3Z2dqJXr17iyy+/1BoISNrKy8uFu7u7ACAuXryotW/Xrl3ipZdeEnZ2dkKpVIr27duLzz//XLMfgNi8ebPWObNnzxYBAQHCzs5OODs7i/79+4tLly4JIXQPvj59+rTo2bOnqF27tnB0dBSdOnXSxPGknwVd7R04cECEhoYKW1tb4ebmJmJjY0VZWZlmf5cuXcT7779v4LcmHVXdf10DcR9+oueBo0ePitatWwtbW1sREhIi1q1bJwBongDTNRDXyclJq43NmzeLh/80PXqtiooKMWfOHOHt7S1sbGxEo0aNtAaJT5o0Sbi4uAgHBwcxdOhQsXDhwkrXIMvFGXGJiKhGpKSk4I033sCtW7eqNWaJ6EnYPUREREbx5ZdfwtfXF56enjh9+jRiY2MxZMgQJixkNExaiIjIKPLy8hAXF4e8vDy4u7tj8ODBWrPVEhmK3UNERERkETiEnoiIiCwCkxYiIiKyCExaiIiIyCIwaSEiIiKLwKSFiIiILAKTFiKqthkzZiAoKEizPmrUKAwYMOCZx3H58mXIZDKkp6dXeUzjxo2xaNGiareZnJyMOnXqGBybTCbDli1bDG6HiCpj0kJk4UaNGgWZTAaZTAYbGxv4+vrio48+QnFxcY1fe/HixUhOTq7WsdVJNIiIHoeTyxFJQO/evbFmzRqUlZXh8OHDGDNmDIqLi7Fy5cpKx+p6i/LTcnJyMko7RETVwUoLkQTI5XK4ubnBy8sLw4cPR1RUlKaL4kGXzhdffAFfX1/I5XIIIXDr1i28/fbbcHV1hVKpxMsvv4zTp09rtfvJJ5+gQYMGcHR0xOjRo3Hv3j2t/Y92D6nVaiQmJqJp06aQy+Vo1KiRZkZUHx8fAEBwcDBkMhnCw8M1561ZswYBAQFQKBRo3rw5VqxYoXWdn3/+GcHBwVAoFGjXrh1OnTql93e0YMECBAYGwt7eHl5eXhg7diyKiooqHbdlyxb4+flBoVCgR48eyM3N1dq/bds2hISEQKFQwNfXFzNnzkR5ebne8RCR/pi0EEmQnZ0dysrKNOsXLlzAhg0bsGnTJk33zCuvvIK8vDzs3LkTaWlpaNu2Lbp164abN28CADZs2ID4+HjMnTsXqampcHd3r5RMPGrq1KlITEzE9OnTce7cOaxbtw4NGjQAcD/xAIAff/wRN27cwHfffQcASEpKwrRp0zB37lxkZmZi3rx5mD59OtauXQsAKC4uRt++feHv74+0tDTMmDEDH330kd7fiZWVFZYsWYKzZ89i7dq1+OmnnzB58mStY+7cuYO5c+di7dq1OHr0KFQqFYYNG6bZv3v3bowYMQLR0dE4d+4cVq9ejeTkZE5VT/SsmPQd00RksJEjR4r+/ftr1k+cOCFcXFzEkCFDhBBCxMfHCxsbG5Gfn685Zt++fUKpVIp79+5ptdWkSROxevVqIYQQHTp0EO+++67W/hdeeEG0adNG57VVKpWQy+UiKSlJZ5zZ2dkCgDh16pTWdi8vL7Fu3TqtbbNnzxYdOnQQQgixevVq4ezsLIqLizX7V65cqbOth3l7e4uFCxdWuX/Dhg3CxcVFs75mzRoBQBw/flyzLTMzUwAQJ06cEEII0alTJzFv3jytdr766ivh7u6uWQcgNm/eXOV1iejpcUwLkQRs374dDg4OKC8vR1lZGfr374+lS5dq9nt7e6N+/fqa9bS0NBQVFcHFxUWrnbt37+LixYsAgMzMTLz77rta+zt06ID9+/frjCEzMxMlJSXo1q1bteMuKChAbm4uRo8ejbfeekuzvby8XDNeJjMzE23atEHt2rW14tDX/v37MW/ePJw7dw4qlQrl5eW4d+8eiouLYW9vDwCoVasW2rVrpzmnefPmqFOnDjIzM9G+fXukpaXhl19+0aqsVFRU4N69e7hz545WjERkfExaiCSga9euWLlyJWxsbODh4VFpoO2DP8oPqNVquLu748CBA5XaetrHfu3s7PQ+R61WA7jfRfTCCy9o7bO2tgYACCO80/XKlSvo06cP3n33XcyePRvOzs44cuQIRo8erdWNBtx/ZPlRD7ap1WrMnDkTAwcOrHSMQqEwOE4iejwmLUQSYG9vj6ZNm1b7+LZt2yIvLw+1atVC48aNdR4TEBCA48eP4/XXX9dsO378eJVtNmvWDHZ2dti3bx/GjBlTab+trS2A+5WJBxo0aABPT09cunQJUVFROttt0aIFvvrqK9y9e1eTGD0uDl1SU1NRXl6Ozz77DFZW94fybdiwodJx5eXlSE1NRfv27QEAWVlZ+Ouvv9C8eXMA97+3rKwsvb5rIjIeJi1Ez6Hu3bujQ4cOGDBgABITE+Hv74/r169j586dGDBgANq1a4f3338fI0eORLt27dCxY0ekpKQgIyMDvr6+OttUKBSIjY3F5MmTYWtri7CwMBQUFCAjIwOjR4+Gq6sr7OzssGvXLjRs2BAKhQJOTk6YMWMGoqOjoVQqERERgZKSEqSmpqKwsBAxMTEYPnw4pk2bhtGjR+Pjjz/G5cuX8emnn+r1eZs0aYLy8nIsXboUkZGROHr0KFatWlXpOBsbG0yYMAFLliyBjY0Nxo8fjxdffFGTxMTFxaFv377w8vLC4MGDYWVlhV9//RVnzpzBnDlz9L8RRKQXPj1E9BySyWTYuXMnOnfujDfffBN+fn4YNmwYLl++rHnaZ+jQoYiLi0NsbCxCQkJw5coVvPfee49td/r06fjwww8RFxeHgIAADB06FPn5+QDujxdZsmQJVq9eDQ8PD/Tv3x8AMGbMGPzzn/9EcnIyAgMD0aVLFyQnJ2sekXZwcMC2bdtw7tw5BAcHY9q0aUhMTNTr8wYFBWHBggVITExEq1atkJKSgoSEhErH1a5dG7GxsRg+fDg6dOgAOzs7rF+/XrO/V69e2L59O/bu3YvQ0FC8+OKLWLBgAby9vfWKh4iejkwYo8OYiIiIqIax0kJEREQWgUkLERERWQQmLURERGQRmLQQERGRRWDSQkRERBaBSQsRERFZBCYtREREZBGYtBAREZFFYNJCREREFoFJCxEREVkEJi1ERERkEf4/2weOFWVIJX0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the cascade classifier on the test data\n",
    "two_stage_predictions = []\n",
    "correct = 0\n",
    "for i in range (test_X.shape[0]):\n",
    "    pred = CascadePredict(test_X[i], setosa_classifier, versicolor_classifier)\n",
    "    two_stage_predictions.append(pred)\n",
    "\n",
    "    if pred == test_y[i]:\n",
    "        correct += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == two_stage_predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")\n",
    "print(f\"Final outcome: {correct} out of {test_X.shape[0]} correct test predictions from Cascaded classifier\")\n",
    "\n",
    "\n",
    "\n",
    "_= ConfusionMatrixDisplay.from_predictions(test_y, two_stage_predictions, display_labels=label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook)</li>\n",
    "    </ol>\n",
    "</div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
